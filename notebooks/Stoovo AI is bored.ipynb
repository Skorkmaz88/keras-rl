{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stoovo1024.png\" width=\"400px\" height=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n",
      "    13/100000: episode: 1, duration: 0.092s, episode steps: 13, steps per second: 141, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.107 [-1.319, 0.772], mean_best_reward: --\n",
      "    24/100000: episode: 2, duration: 0.007s, episode steps: 11, steps per second: 1535, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.151 [-0.941, 1.832], mean_best_reward: --\n",
      "    37/100000: episode: 3, duration: 0.008s, episode steps: 13, steps per second: 1606, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.134 [-1.324, 2.292], mean_best_reward: --\n",
      "    51/100000: episode: 4, duration: 0.008s, episode steps: 14, steps per second: 1682, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.098 [-1.204, 2.050], mean_best_reward: --\n",
      "    68/100000: episode: 5, duration: 0.013s, episode steps: 17, steps per second: 1299, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.087 [-1.945, 1.031], mean_best_reward: --\n",
      "    82/100000: episode: 6, duration: 0.009s, episode steps: 14, steps per second: 1628, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.116 [-0.788, 1.541], mean_best_reward: --\n",
      "   102/100000: episode: 7, duration: 0.011s, episode steps: 20, steps per second: 1748, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.086 [-0.747, 1.392], mean_best_reward: --\n",
      "   116/100000: episode: 8, duration: 0.010s, episode steps: 14, steps per second: 1376, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.084 [-1.549, 0.975], mean_best_reward: --\n",
      "   127/100000: episode: 9, duration: 0.012s, episode steps: 11, steps per second: 894, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.091 [-1.886, 1.222], mean_best_reward: --\n",
      "   156/100000: episode: 10, duration: 0.019s, episode steps: 29, steps per second: 1532, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.655 [0.000, 1.000], mean observation: -0.011 [-2.438, 1.730], mean_best_reward: --\n",
      "   168/100000: episode: 11, duration: 0.007s, episode steps: 12, steps per second: 1622, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.117 [-1.932, 1.150], mean_best_reward: --\n",
      "   183/100000: episode: 12, duration: 0.009s, episode steps: 15, steps per second: 1581, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.100 [-1.333, 2.274], mean_best_reward: --\n",
      "   196/100000: episode: 13, duration: 0.009s, episode steps: 13, steps per second: 1520, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.113 [-1.718, 2.721], mean_best_reward: --\n",
      "   208/100000: episode: 14, duration: 0.007s, episode steps: 12, steps per second: 1612, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.120 [-1.354, 2.185], mean_best_reward: --\n",
      "   217/100000: episode: 15, duration: 0.006s, episode steps: 9, steps per second: 1477, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.121 [-1.379, 2.248], mean_best_reward: --\n",
      "   227/100000: episode: 16, duration: 0.006s, episode steps: 10, steps per second: 1564, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.123 [-2.436, 1.572], mean_best_reward: --\n",
      "   242/100000: episode: 17, duration: 0.009s, episode steps: 15, steps per second: 1744, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.113 [-1.360, 0.622], mean_best_reward: --\n",
      "   256/100000: episode: 18, duration: 0.010s, episode steps: 14, steps per second: 1415, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.100 [-1.190, 1.996], mean_best_reward: --\n",
      "   270/100000: episode: 19, duration: 0.010s, episode steps: 14, steps per second: 1374, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.081 [-1.519, 0.819], mean_best_reward: --\n",
      "   283/100000: episode: 20, duration: 0.008s, episode steps: 13, steps per second: 1683, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.101 [-1.747, 0.992], mean_best_reward: --\n",
      "   299/100000: episode: 21, duration: 0.010s, episode steps: 16, steps per second: 1598, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.117 [-0.576, 1.184], mean_best_reward: --\n",
      "   314/100000: episode: 22, duration: 0.011s, episode steps: 15, steps per second: 1328, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.992, 1.590], mean_best_reward: --\n",
      "   354/100000: episode: 23, duration: 0.029s, episode steps: 40, steps per second: 1359, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.110 [-0.888, 1.608], mean_best_reward: --\n",
      "   364/100000: episode: 24, duration: 0.008s, episode steps: 10, steps per second: 1321, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.099, 1.970], mean_best_reward: --\n",
      "   378/100000: episode: 25, duration: 0.012s, episode steps: 14, steps per second: 1160, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.066 [-1.383, 2.121], mean_best_reward: --\n",
      "   397/100000: episode: 26, duration: 0.017s, episode steps: 19, steps per second: 1143, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.045 [-2.581, 1.769], mean_best_reward: --\n",
      "   428/100000: episode: 27, duration: 0.025s, episode steps: 31, steps per second: 1233, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.117 [-2.221, 0.996], mean_best_reward: --\n",
      "   451/100000: episode: 28, duration: 0.015s, episode steps: 23, steps per second: 1571, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.053 [-0.844, 1.576], mean_best_reward: --\n",
      "   465/100000: episode: 29, duration: 0.012s, episode steps: 14, steps per second: 1182, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.071 [-1.293, 0.831], mean_best_reward: --\n",
      "   482/100000: episode: 30, duration: 0.016s, episode steps: 17, steps per second: 1077, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.072 [-1.722, 1.030], mean_best_reward: --\n",
      "   504/100000: episode: 31, duration: 0.016s, episode steps: 22, steps per second: 1368, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.103 [-1.745, 0.773], mean_best_reward: --\n",
      "   529/100000: episode: 32, duration: 0.020s, episode steps: 25, steps per second: 1222, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.062 [-2.220, 1.329], mean_best_reward: --\n",
      "   549/100000: episode: 33, duration: 0.013s, episode steps: 20, steps per second: 1491, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.091 [-1.232, 0.804], mean_best_reward: --\n",
      "   565/100000: episode: 34, duration: 0.015s, episode steps: 16, steps per second: 1076, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.098 [-1.133, 2.087], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   652/100000: episode: 35, duration: 0.065s, episode steps: 87, steps per second: 1338, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.164 [-1.532, 1.254], mean_best_reward: --\n",
      "   672/100000: episode: 36, duration: 0.020s, episode steps: 20, steps per second: 1011, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.053 [-1.201, 1.977], mean_best_reward: --\n",
      "   716/100000: episode: 37, duration: 0.035s, episode steps: 44, steps per second: 1261, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.568 [0.000, 1.000], mean observation: -0.055 [-2.395, 1.395], mean_best_reward: --\n",
      "   728/100000: episode: 38, duration: 0.008s, episode steps: 12, steps per second: 1480, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.107 [-0.774, 1.524], mean_best_reward: --\n",
      "   742/100000: episode: 39, duration: 0.010s, episode steps: 14, steps per second: 1367, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.106 [-2.619, 1.567], mean_best_reward: --\n",
      "   795/100000: episode: 40, duration: 0.042s, episode steps: 53, steps per second: 1261, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.020 [-1.309, 0.945], mean_best_reward: --\n",
      "   816/100000: episode: 41, duration: 0.020s, episode steps: 21, steps per second: 1031, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.069 [-1.439, 0.979], mean_best_reward: --\n",
      "   833/100000: episode: 42, duration: 0.018s, episode steps: 17, steps per second: 962, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.106 [-1.847, 0.944], mean_best_reward: --\n",
      "   883/100000: episode: 43, duration: 0.051s, episode steps: 50, steps per second: 984, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.254 [-0.785, 1.712], mean_best_reward: --\n",
      "   893/100000: episode: 44, duration: 0.010s, episode steps: 10, steps per second: 1022, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.145 [-1.523, 2.391], mean_best_reward: --\n",
      "   924/100000: episode: 45, duration: 0.020s, episode steps: 31, steps per second: 1527, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.613 [0.000, 1.000], mean observation: -0.034 [-2.329, 1.418], mean_best_reward: --\n",
      "   939/100000: episode: 46, duration: 0.010s, episode steps: 15, steps per second: 1524, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.066 [-2.251, 1.410], mean_best_reward: --\n",
      "   955/100000: episode: 47, duration: 0.010s, episode steps: 16, steps per second: 1614, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.060 [-2.520, 1.606], mean_best_reward: --\n",
      "   998/100000: episode: 48, duration: 0.026s, episode steps: 43, steps per second: 1630, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.003 [-1.143, 1.426], mean_best_reward: --\n",
      "  1019/100000: episode: 49, duration: 0.018s, episode steps: 21, steps per second: 1200, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.076 [-1.190, 2.051], mean_best_reward: --\n",
      "  1030/100000: episode: 50, duration: 0.008s, episode steps: 11, steps per second: 1369, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.101 [-1.197, 1.872], mean_best_reward: --\n",
      "  1045/100000: episode: 51, duration: 0.011s, episode steps: 15, steps per second: 1310, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.093 [-0.954, 1.767], mean_best_reward: --\n",
      "  1059/100000: episode: 52, duration: 0.010s, episode steps: 14, steps per second: 1374, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.119 [-2.666, 1.539], mean_best_reward: --\n",
      "  1075/100000: episode: 53, duration: 0.010s, episode steps: 16, steps per second: 1573, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.075 [-1.607, 2.480], mean_best_reward: --\n",
      "  1091/100000: episode: 54, duration: 0.011s, episode steps: 16, steps per second: 1476, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.081 [-2.094, 1.362], mean_best_reward: --\n",
      "  1102/100000: episode: 55, duration: 0.009s, episode steps: 11, steps per second: 1180, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.113 [-1.155, 1.851], mean_best_reward: --\n",
      "  1115/100000: episode: 56, duration: 0.009s, episode steps: 13, steps per second: 1392, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.080 [-1.397, 2.108], mean_best_reward: --\n",
      "  1126/100000: episode: 57, duration: 0.008s, episode steps: 11, steps per second: 1407, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.121 [-2.142, 1.356], mean_best_reward: --\n",
      "  1149/100000: episode: 58, duration: 0.015s, episode steps: 23, steps per second: 1569, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.064 [-0.566, 1.110], mean_best_reward: --\n",
      "  1202/100000: episode: 59, duration: 0.038s, episode steps: 53, steps per second: 1382, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.002 [-1.379, 0.753], mean_best_reward: --\n",
      "  1216/100000: episode: 60, duration: 0.011s, episode steps: 14, steps per second: 1285, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.093 [-1.609, 2.527], mean_best_reward: --\n",
      "  1254/100000: episode: 61, duration: 0.053s, episode steps: 38, steps per second: 719, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.061 [-1.214, 0.587], mean_best_reward: --\n",
      "  1271/100000: episode: 62, duration: 0.035s, episode steps: 17, steps per second: 491, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.096 [-1.814, 0.995], mean_best_reward: --\n",
      "  1370/100000: episode: 63, duration: 0.163s, episode steps: 99, steps per second: 607, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.085 [-1.246, 1.047], mean_best_reward: --\n",
      "  1389/100000: episode: 64, duration: 0.019s, episode steps: 19, steps per second: 980, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.080 [-0.740, 1.199], mean_best_reward: --\n",
      "  1459/100000: episode: 65, duration: 0.038s, episode steps: 70, steps per second: 1855, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.107 [-2.246, 1.724], mean_best_reward: --\n",
      "  1475/100000: episode: 66, duration: 0.009s, episode steps: 16, steps per second: 1702, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.095 [-1.595, 0.815], mean_best_reward: --\n",
      "  1489/100000: episode: 67, duration: 0.008s, episode steps: 14, steps per second: 1703, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.078 [-1.181, 1.906], mean_best_reward: --\n",
      "  1513/100000: episode: 68, duration: 0.013s, episode steps: 24, steps per second: 1822, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.076 [-0.776, 1.601], mean_best_reward: --\n",
      "  1532/100000: episode: 69, duration: 0.011s, episode steps: 19, steps per second: 1750, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.069 [-0.780, 1.464], mean_best_reward: --\n",
      "  1541/100000: episode: 70, duration: 0.006s, episode steps: 9, steps per second: 1528, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.119 [-2.130, 1.384], mean_best_reward: --\n",
      "  1568/100000: episode: 71, duration: 0.015s, episode steps: 27, steps per second: 1763, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.008 [-1.488, 0.931], mean_best_reward: --\n",
      "  1577/100000: episode: 72, duration: 0.008s, episode steps: 9, steps per second: 1172, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.154 [-1.148, 1.991], mean_best_reward: --\n",
      "  1591/100000: episode: 73, duration: 0.009s, episode steps: 14, steps per second: 1573, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.086 [-1.613, 2.588], mean_best_reward: --\n",
      "  1622/100000: episode: 74, duration: 0.027s, episode steps: 31, steps per second: 1149, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.160 [-0.540, 1.168], mean_best_reward: --\n",
      "  1638/100000: episode: 75, duration: 0.024s, episode steps: 16, steps per second: 673, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.093 [-0.963, 1.647], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1651/100000: episode: 76, duration: 0.021s, episode steps: 13, steps per second: 629, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.125 [-0.935, 1.534], mean_best_reward: --\n",
      "  1686/100000: episode: 77, duration: 0.031s, episode steps: 35, steps per second: 1133, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: -0.053 [-1.702, 1.962], mean_best_reward: --\n",
      "  1707/100000: episode: 78, duration: 0.026s, episode steps: 21, steps per second: 822, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.041 [-1.700, 1.027], mean_best_reward: --\n",
      "  1720/100000: episode: 79, duration: 0.013s, episode steps: 13, steps per second: 986, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.094 [-1.725, 1.021], mean_best_reward: --\n",
      "  1729/100000: episode: 80, duration: 0.010s, episode steps: 9, steps per second: 868, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.136 [-2.446, 1.607], mean_best_reward: --\n",
      "  1741/100000: episode: 81, duration: 0.014s, episode steps: 12, steps per second: 839, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.117 [-1.355, 2.126], mean_best_reward: --\n",
      "  1752/100000: episode: 82, duration: 0.014s, episode steps: 11, steps per second: 777, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.329, 1.418], mean_best_reward: --\n",
      "  1771/100000: episode: 83, duration: 0.038s, episode steps: 19, steps per second: 504, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.084 [-1.328, 0.762], mean_best_reward: --\n",
      "  1795/100000: episode: 84, duration: 0.039s, episode steps: 24, steps per second: 623, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.541, 1.014], mean_best_reward: --\n",
      "  1814/100000: episode: 85, duration: 0.034s, episode steps: 19, steps per second: 554, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.075 [-1.479, 0.966], mean_best_reward: --\n",
      "  1847/100000: episode: 86, duration: 0.049s, episode steps: 33, steps per second: 673, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.060 [-0.404, 1.106], mean_best_reward: --\n",
      "  1859/100000: episode: 87, duration: 0.016s, episode steps: 12, steps per second: 737, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.135 [-1.521, 2.544], mean_best_reward: --\n",
      "  1874/100000: episode: 88, duration: 0.022s, episode steps: 15, steps per second: 669, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.100 [-2.323, 1.341], mean_best_reward: --\n",
      "  1889/100000: episode: 89, duration: 0.017s, episode steps: 15, steps per second: 872, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-1.206, 2.017], mean_best_reward: --\n",
      "  1899/100000: episode: 90, duration: 0.015s, episode steps: 10, steps per second: 663, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.123 [-1.928, 1.217], mean_best_reward: --\n",
      "  1918/100000: episode: 91, duration: 0.026s, episode steps: 19, steps per second: 733, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.088 [-2.458, 1.429], mean_best_reward: --\n",
      "  1938/100000: episode: 92, duration: 0.023s, episode steps: 20, steps per second: 876, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.070 [-1.205, 2.036], mean_best_reward: --\n",
      "  1974/100000: episode: 93, duration: 0.035s, episode steps: 36, steps per second: 1035, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.064 [-1.197, 2.305], mean_best_reward: --\n",
      "  1991/100000: episode: 94, duration: 0.025s, episode steps: 17, steps per second: 693, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.089 [-1.089, 0.573], mean_best_reward: --\n",
      "  1999/100000: episode: 95, duration: 0.017s, episode steps: 8, steps per second: 480, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.160 [-1.142, 2.028], mean_best_reward: --\n",
      "  2009/100000: episode: 96, duration: 0.016s, episode steps: 10, steps per second: 627, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.520, 2.502], mean_best_reward: --\n",
      "  2029/100000: episode: 97, duration: 0.030s, episode steps: 20, steps per second: 665, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-0.930, 0.391], mean_best_reward: --\n",
      "  2043/100000: episode: 98, duration: 0.021s, episode steps: 14, steps per second: 670, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.106 [-1.156, 2.137], mean_best_reward: --\n",
      "  2075/100000: episode: 99, duration: 0.051s, episode steps: 32, steps per second: 625, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.053 [-1.194, 0.590], mean_best_reward: --\n",
      "  2092/100000: episode: 100, duration: 0.026s, episode steps: 17, steps per second: 650, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.044 [-0.827, 1.272], mean_best_reward: --\n",
      "  2114/100000: episode: 101, duration: 0.028s, episode steps: 22, steps per second: 776, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.050 [-1.562, 2.549], mean_best_reward: 51.500000\n",
      "  2135/100000: episode: 102, duration: 0.018s, episode steps: 21, steps per second: 1177, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.103 [-1.202, 0.578], mean_best_reward: --\n",
      "  2198/100000: episode: 103, duration: 0.038s, episode steps: 63, steps per second: 1649, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.050 [-1.765, 1.906], mean_best_reward: --\n",
      "  2211/100000: episode: 104, duration: 0.010s, episode steps: 13, steps per second: 1356, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.097 [-0.595, 1.163], mean_best_reward: --\n",
      "  2225/100000: episode: 105, duration: 0.009s, episode steps: 14, steps per second: 1639, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.115 [-2.661, 1.555], mean_best_reward: --\n",
      "  2286/100000: episode: 106, duration: 0.034s, episode steps: 61, steps per second: 1796, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.039 [-1.089, 0.951], mean_best_reward: --\n",
      "  2297/100000: episode: 107, duration: 0.007s, episode steps: 11, steps per second: 1509, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.106 [-2.293, 1.390], mean_best_reward: --\n",
      "  2323/100000: episode: 108, duration: 0.015s, episode steps: 26, steps per second: 1743, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.087 [-0.754, 1.595], mean_best_reward: --\n",
      "  2340/100000: episode: 109, duration: 0.012s, episode steps: 17, steps per second: 1477, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.055 [-2.646, 1.790], mean_best_reward: --\n",
      "  2367/100000: episode: 110, duration: 0.020s, episode steps: 27, steps per second: 1372, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.097 [-0.961, 0.588], mean_best_reward: --\n",
      "  2378/100000: episode: 111, duration: 0.011s, episode steps: 11, steps per second: 990, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.123 [-2.448, 1.554], mean_best_reward: --\n",
      "  2405/100000: episode: 112, duration: 0.021s, episode steps: 27, steps per second: 1311, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.055 [-0.772, 1.409], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2418/100000: episode: 113, duration: 0.015s, episode steps: 13, steps per second: 847, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.120 [-0.756, 1.251], mean_best_reward: --\n",
      "  2433/100000: episode: 114, duration: 0.015s, episode steps: 15, steps per second: 1032, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.093 [-2.383, 1.529], mean_best_reward: --\n",
      "  2465/100000: episode: 115, duration: 0.022s, episode steps: 32, steps per second: 1427, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.063 [-0.842, 1.785], mean_best_reward: --\n",
      "  2474/100000: episode: 116, duration: 0.008s, episode steps: 9, steps per second: 1198, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.138 [-1.203, 1.869], mean_best_reward: --\n",
      "  2485/100000: episode: 117, duration: 0.008s, episode steps: 11, steps per second: 1317, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.123 [-1.330, 0.794], mean_best_reward: --\n",
      "  2498/100000: episode: 118, duration: 0.012s, episode steps: 13, steps per second: 1122, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.103 [-1.976, 2.964], mean_best_reward: --\n",
      "  2534/100000: episode: 119, duration: 0.022s, episode steps: 36, steps per second: 1618, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.158 [-1.313, 0.647], mean_best_reward: --\n",
      "  2548/100000: episode: 120, duration: 0.011s, episode steps: 14, steps per second: 1227, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.101 [-1.787, 1.032], mean_best_reward: --\n",
      "  2565/100000: episode: 121, duration: 0.014s, episode steps: 17, steps per second: 1255, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.117 [-1.766, 0.951], mean_best_reward: --\n",
      "  2577/100000: episode: 122, duration: 0.011s, episode steps: 12, steps per second: 1083, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.090 [-1.825, 1.161], mean_best_reward: --\n",
      "  2683/100000: episode: 123, duration: 0.072s, episode steps: 106, steps per second: 1479, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.042 [-1.188, 1.099], mean_best_reward: --\n",
      "  2705/100000: episode: 124, duration: 0.024s, episode steps: 22, steps per second: 928, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.588, 1.004], mean_best_reward: --\n",
      "  2726/100000: episode: 125, duration: 0.013s, episode steps: 21, steps per second: 1617, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.069 [-2.306, 1.370], mean_best_reward: --\n",
      "  2737/100000: episode: 126, duration: 0.007s, episode steps: 11, steps per second: 1619, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.125 [-1.836, 1.017], mean_best_reward: --\n",
      "  2759/100000: episode: 127, duration: 0.013s, episode steps: 22, steps per second: 1681, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.102 [-1.070, 0.627], mean_best_reward: --\n",
      "  2780/100000: episode: 128, duration: 0.013s, episode steps: 21, steps per second: 1623, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.031 [-2.037, 1.412], mean_best_reward: --\n",
      "  2794/100000: episode: 129, duration: 0.010s, episode steps: 14, steps per second: 1381, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.092 [-2.432, 1.545], mean_best_reward: --\n",
      "  2805/100000: episode: 130, duration: 0.010s, episode steps: 11, steps per second: 1050, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.128 [-1.201, 2.003], mean_best_reward: --\n",
      "  2822/100000: episode: 131, duration: 0.015s, episode steps: 17, steps per second: 1109, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.070 [-1.006, 1.771], mean_best_reward: --\n",
      "  2853/100000: episode: 132, duration: 0.022s, episode steps: 31, steps per second: 1392, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.007 [-1.661, 0.998], mean_best_reward: --\n",
      "  2871/100000: episode: 133, duration: 0.016s, episode steps: 18, steps per second: 1152, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.085 [-0.756, 1.325], mean_best_reward: --\n",
      "  2898/100000: episode: 134, duration: 0.018s, episode steps: 27, steps per second: 1495, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.080 [-1.676, 0.824], mean_best_reward: --\n",
      "  2911/100000: episode: 135, duration: 0.011s, episode steps: 13, steps per second: 1214, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.114 [-1.947, 1.162], mean_best_reward: --\n",
      "  2923/100000: episode: 136, duration: 0.011s, episode steps: 12, steps per second: 1102, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.089 [-1.019, 1.523], mean_best_reward: --\n",
      "  2943/100000: episode: 137, duration: 0.015s, episode steps: 20, steps per second: 1358, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.082 [-1.792, 0.991], mean_best_reward: --\n",
      "  2983/100000: episode: 138, duration: 0.029s, episode steps: 40, steps per second: 1372, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.167, 0.744], mean_best_reward: --\n",
      "  3013/100000: episode: 139, duration: 0.019s, episode steps: 30, steps per second: 1578, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.024 [-1.778, 1.126], mean_best_reward: --\n",
      "  3024/100000: episode: 140, duration: 0.007s, episode steps: 11, steps per second: 1479, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.121 [-1.512, 0.981], mean_best_reward: --\n",
      "  3045/100000: episode: 141, duration: 0.013s, episode steps: 21, steps per second: 1582, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.078 [-2.371, 1.358], mean_best_reward: --\n",
      "  3063/100000: episode: 142, duration: 0.013s, episode steps: 18, steps per second: 1391, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.042 [-1.182, 1.717], mean_best_reward: --\n",
      "  3074/100000: episode: 143, duration: 0.009s, episode steps: 11, steps per second: 1236, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.158 [-0.944, 1.845], mean_best_reward: --\n",
      "  3117/100000: episode: 144, duration: 0.033s, episode steps: 43, steps per second: 1297, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.031 [-1.252, 0.753], mean_best_reward: --\n",
      "  3161/100000: episode: 145, duration: 0.029s, episode steps: 44, steps per second: 1495, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.123 [-0.929, 1.810], mean_best_reward: --\n",
      "  3183/100000: episode: 146, duration: 0.020s, episode steps: 22, steps per second: 1086, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.105 [-1.689, 0.781], mean_best_reward: --\n",
      "  3199/100000: episode: 147, duration: 0.018s, episode steps: 16, steps per second: 877, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-0.969, 0.552], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3220/100000: episode: 148, duration: 0.020s, episode steps: 21, steps per second: 1054, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.091 [-0.990, 1.892], mean_best_reward: --\n",
      "  3233/100000: episode: 149, duration: 0.010s, episode steps: 13, steps per second: 1257, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.113 [-2.431, 1.383], mean_best_reward: --\n",
      "  3277/100000: episode: 150, duration: 0.025s, episode steps: 44, steps per second: 1777, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.077 [-2.491, 1.554], mean_best_reward: --\n",
      "  3290/100000: episode: 151, duration: 0.009s, episode steps: 13, steps per second: 1433, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.094 [-1.175, 1.901], mean_best_reward: 65.500000\n",
      "  3306/100000: episode: 152, duration: 0.014s, episode steps: 16, steps per second: 1179, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.075 [-0.840, 1.484], mean_best_reward: --\n",
      "  3320/100000: episode: 153, duration: 0.016s, episode steps: 14, steps per second: 854, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.106 [-1.175, 2.060], mean_best_reward: --\n",
      "  3410/100000: episode: 154, duration: 0.084s, episode steps: 90, steps per second: 1068, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.092 [-1.162, 1.039], mean_best_reward: --\n",
      "  3453/100000: episode: 155, duration: 0.046s, episode steps: 43, steps per second: 931, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.003 [-1.272, 0.796], mean_best_reward: --\n",
      "  3464/100000: episode: 156, duration: 0.015s, episode steps: 11, steps per second: 722, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.132 [-1.780, 2.815], mean_best_reward: --\n",
      "  3496/100000: episode: 157, duration: 0.039s, episode steps: 32, steps per second: 811, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.011 [-2.489, 1.728], mean_best_reward: --\n",
      "  3608/100000: episode: 158, duration: 0.085s, episode steps: 112, steps per second: 1310, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.236 [-1.444, 1.891], mean_best_reward: --\n",
      "  3618/100000: episode: 159, duration: 0.009s, episode steps: 10, steps per second: 1125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.128 [-1.539, 2.571], mean_best_reward: --\n",
      "  3637/100000: episode: 160, duration: 0.015s, episode steps: 19, steps per second: 1279, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.059 [-1.655, 1.034], mean_best_reward: --\n",
      "  3648/100000: episode: 161, duration: 0.010s, episode steps: 11, steps per second: 1088, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.127 [-0.998, 1.807], mean_best_reward: --\n",
      "  3666/100000: episode: 162, duration: 0.026s, episode steps: 18, steps per second: 689, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.052 [-1.281, 0.807], mean_best_reward: --\n",
      "  3689/100000: episode: 163, duration: 0.032s, episode steps: 23, steps per second: 718, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.093 [-1.119, 0.548], mean_best_reward: --\n",
      "  3797/100000: episode: 164, duration: 0.110s, episode steps: 108, steps per second: 983, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.052 [-1.143, 1.304], mean_best_reward: --\n",
      "  3843/100000: episode: 165, duration: 0.046s, episode steps: 46, steps per second: 1009, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.112 [-2.170, 0.798], mean_best_reward: --\n",
      "  3892/100000: episode: 166, duration: 0.049s, episode steps: 49, steps per second: 1006, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: 0.006 [-1.220, 1.815], mean_best_reward: --\n",
      "  3921/100000: episode: 167, duration: 0.040s, episode steps: 29, steps per second: 717, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.084 [-2.031, 1.011], mean_best_reward: --\n",
      "  3942/100000: episode: 168, duration: 0.031s, episode steps: 21, steps per second: 685, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.068 [-1.901, 1.035], mean_best_reward: --\n",
      "  4032/100000: episode: 169, duration: 0.101s, episode steps: 90, steps per second: 891, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.184 [-1.331, 1.088], mean_best_reward: --\n",
      "  4048/100000: episode: 170, duration: 0.013s, episode steps: 16, steps per second: 1262, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.096 [-0.766, 1.307], mean_best_reward: --\n",
      "  4066/100000: episode: 171, duration: 0.013s, episode steps: 18, steps per second: 1407, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.479, 0.939], mean_best_reward: --\n",
      "  4082/100000: episode: 172, duration: 0.014s, episode steps: 16, steps per second: 1151, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.105 [-0.945, 1.654], mean_best_reward: --\n",
      "  4098/100000: episode: 173, duration: 0.017s, episode steps: 16, steps per second: 939, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.091 [-1.134, 1.941], mean_best_reward: --\n",
      "  4114/100000: episode: 174, duration: 0.010s, episode steps: 16, steps per second: 1591, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.080 [-1.624, 2.674], mean_best_reward: --\n",
      "  4197/100000: episode: 175, duration: 0.063s, episode steps: 83, steps per second: 1322, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.236 [-1.450, 2.091], mean_best_reward: --\n",
      "  4212/100000: episode: 176, duration: 0.013s, episode steps: 15, steps per second: 1177, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.063 [-1.393, 2.171], mean_best_reward: --\n",
      "  4227/100000: episode: 177, duration: 0.014s, episode steps: 15, steps per second: 1061, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.099 [-0.959, 1.469], mean_best_reward: --\n",
      "  4266/100000: episode: 178, duration: 0.028s, episode steps: 39, steps per second: 1408, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.071 [-1.488, 0.763], mean_best_reward: --\n",
      "  4277/100000: episode: 179, duration: 0.008s, episode steps: 11, steps per second: 1313, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.122 [-1.378, 2.298], mean_best_reward: --\n",
      "  4293/100000: episode: 180, duration: 0.009s, episode steps: 16, steps per second: 1705, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.073 [-1.543, 2.260], mean_best_reward: --\n",
      "  4309/100000: episode: 181, duration: 0.010s, episode steps: 16, steps per second: 1664, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.091 [-2.134, 1.212], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4359/100000: episode: 182, duration: 0.046s, episode steps: 50, steps per second: 1090, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.164 [-1.251, 0.554], mean_best_reward: --\n",
      "  4371/100000: episode: 183, duration: 0.008s, episode steps: 12, steps per second: 1593, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.099 [-1.697, 0.968], mean_best_reward: --\n",
      "  4409/100000: episode: 184, duration: 0.022s, episode steps: 38, steps per second: 1760, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.130 [-0.863, 0.282], mean_best_reward: --\n",
      "  4443/100000: episode: 185, duration: 0.019s, episode steps: 34, steps per second: 1782, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.060 [-0.968, 1.695], mean_best_reward: --\n",
      "  4470/100000: episode: 186, duration: 0.016s, episode steps: 27, steps per second: 1709, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.026 [-1.383, 0.969], mean_best_reward: --\n",
      "  4514/100000: episode: 187, duration: 0.024s, episode steps: 44, steps per second: 1853, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.034 [-1.153, 1.508], mean_best_reward: --\n",
      "  4560/100000: episode: 188, duration: 0.032s, episode steps: 46, steps per second: 1441, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.212, 0.417], mean_best_reward: --\n",
      "  4586/100000: episode: 189, duration: 0.024s, episode steps: 26, steps per second: 1100, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.068 [-1.198, 2.017], mean_best_reward: --\n",
      "  4623/100000: episode: 190, duration: 0.027s, episode steps: 37, steps per second: 1385, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.081 [-1.062, 0.609], mean_best_reward: --\n",
      "  4639/100000: episode: 191, duration: 0.013s, episode steps: 16, steps per second: 1213, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.083 [-1.999, 1.181], mean_best_reward: --\n",
      "  4685/100000: episode: 192, duration: 0.039s, episode steps: 46, steps per second: 1180, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.777, 1.314], mean_best_reward: --\n",
      "  4716/100000: episode: 193, duration: 0.018s, episode steps: 31, steps per second: 1706, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.083 [-1.131, 0.579], mean_best_reward: --\n",
      "  4751/100000: episode: 194, duration: 0.028s, episode steps: 35, steps per second: 1256, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.037 [-1.423, 0.949], mean_best_reward: --\n",
      "  4800/100000: episode: 195, duration: 0.031s, episode steps: 49, steps per second: 1564, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.105 [-1.232, 0.506], mean_best_reward: --\n",
      "  4839/100000: episode: 196, duration: 0.022s, episode steps: 39, steps per second: 1796, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.064 [-1.946, 1.772], mean_best_reward: --\n",
      "  4869/100000: episode: 197, duration: 0.021s, episode steps: 30, steps per second: 1402, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.242, 0.396], mean_best_reward: --\n",
      "  4961/100000: episode: 198, duration: 0.082s, episode steps: 92, steps per second: 1122, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.115 [-0.638, 1.076], mean_best_reward: --\n",
      "  4971/100000: episode: 199, duration: 0.011s, episode steps: 10, steps per second: 920, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.138 [-1.156, 2.050], mean_best_reward: --\n",
      "  5017/100000: episode: 200, duration: 0.028s, episode steps: 46, steps per second: 1626, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.078 [-1.499, 0.767], mean_best_reward: --\n",
      "  5037/100000: episode: 201, duration: 0.016s, episode steps: 20, steps per second: 1290, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.102 [-0.769, 1.576], mean_best_reward: 79.000000\n",
      "  5055/100000: episode: 202, duration: 0.023s, episode steps: 18, steps per second: 799, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.083 [-0.840, 1.423], mean_best_reward: --\n",
      "  5073/100000: episode: 203, duration: 0.026s, episode steps: 18, steps per second: 687, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.086 [-0.987, 1.609], mean_best_reward: --\n",
      "  5127/100000: episode: 204, duration: 0.087s, episode steps: 54, steps per second: 618, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.114 [-1.401, 1.383], mean_best_reward: --\n",
      "  5194/100000: episode: 205, duration: 0.095s, episode steps: 67, steps per second: 708, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.171 [-1.077, 1.929], mean_best_reward: --\n",
      "  5212/100000: episode: 206, duration: 0.018s, episode steps: 18, steps per second: 1013, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.130 [-0.542, 1.232], mean_best_reward: --\n",
      "  5231/100000: episode: 207, duration: 0.014s, episode steps: 19, steps per second: 1325, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.093 [-0.628, 1.381], mean_best_reward: --\n",
      "  5274/100000: episode: 208, duration: 0.031s, episode steps: 43, steps per second: 1407, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.081 [-0.632, 1.808], mean_best_reward: --\n",
      "  5300/100000: episode: 209, duration: 0.034s, episode steps: 26, steps per second: 754, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.085 [-0.735, 1.127], mean_best_reward: --\n",
      "  5322/100000: episode: 210, duration: 0.022s, episode steps: 22, steps per second: 985, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.046 [-1.228, 0.735], mean_best_reward: --\n",
      "  5337/100000: episode: 211, duration: 0.018s, episode steps: 15, steps per second: 834, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.081 [-0.827, 1.394], mean_best_reward: --\n",
      "  5409/100000: episode: 212, duration: 0.073s, episode steps: 72, steps per second: 992, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.056 [-2.071, 1.142], mean_best_reward: --\n",
      "  5432/100000: episode: 213, duration: 0.021s, episode steps: 23, steps per second: 1081, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.105 [-0.748, 1.184], mean_best_reward: --\n",
      "  5443/100000: episode: 214, duration: 0.016s, episode steps: 11, steps per second: 684, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.120 [-0.776, 1.405], mean_best_reward: --\n",
      "  5483/100000: episode: 215, duration: 0.043s, episode steps: 40, steps per second: 929, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.027 [-0.946, 1.477], mean_best_reward: --\n",
      "  5509/100000: episode: 216, duration: 0.034s, episode steps: 26, steps per second: 755, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.045 [-2.004, 1.181], mean_best_reward: --\n",
      "  5540/100000: episode: 217, duration: 0.039s, episode steps: 31, steps per second: 804, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: 0.018 [-1.794, 1.515], mean_best_reward: --\n",
      "  5553/100000: episode: 218, duration: 0.016s, episode steps: 13, steps per second: 792, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.094 [-1.023, 1.506], mean_best_reward: --\n",
      "  5566/100000: episode: 219, duration: 0.020s, episode steps: 13, steps per second: 643, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.091 [-1.743, 2.714], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5605/100000: episode: 220, duration: 0.057s, episode steps: 39, steps per second: 684, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.058 [-0.609, 1.541], mean_best_reward: --\n",
      "  5650/100000: episode: 221, duration: 0.041s, episode steps: 45, steps per second: 1103, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.035 [-1.272, 0.834], mean_best_reward: --\n",
      "  5669/100000: episode: 222, duration: 0.012s, episode steps: 19, steps per second: 1556, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.108 [-1.201, 0.768], mean_best_reward: --\n",
      "  5688/100000: episode: 223, duration: 0.012s, episode steps: 19, steps per second: 1620, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.077 [-1.431, 0.824], mean_best_reward: --\n",
      "  5718/100000: episode: 224, duration: 0.022s, episode steps: 30, steps per second: 1378, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.612, 0.972], mean_best_reward: --\n",
      "  5741/100000: episode: 225, duration: 0.019s, episode steps: 23, steps per second: 1206, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.100 [-0.768, 0.444], mean_best_reward: --\n",
      "  5802/100000: episode: 226, duration: 0.058s, episode steps: 61, steps per second: 1044, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.202 [-0.784, 1.127], mean_best_reward: --\n",
      "  5866/100000: episode: 227, duration: 0.070s, episode steps: 64, steps per second: 918, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.209 [-1.752, 0.722], mean_best_reward: --\n",
      "  5887/100000: episode: 228, duration: 0.018s, episode steps: 21, steps per second: 1177, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.071 [-1.158, 0.583], mean_best_reward: --\n",
      "  5901/100000: episode: 229, duration: 0.020s, episode steps: 14, steps per second: 691, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.075 [-1.551, 1.000], mean_best_reward: --\n",
      "  5953/100000: episode: 230, duration: 0.041s, episode steps: 52, steps per second: 1277, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.023 [-0.953, 1.339], mean_best_reward: --\n",
      "  6033/100000: episode: 231, duration: 0.045s, episode steps: 80, steps per second: 1782, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.177 [-1.643, 1.365], mean_best_reward: --\n",
      "  6083/100000: episode: 232, duration: 0.028s, episode steps: 50, steps per second: 1781, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.137 [-0.781, 1.376], mean_best_reward: --\n",
      "  6111/100000: episode: 233, duration: 0.022s, episode steps: 28, steps per second: 1298, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.139 [-1.085, 0.351], mean_best_reward: --\n",
      "  6127/100000: episode: 234, duration: 0.011s, episode steps: 16, steps per second: 1446, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.793, 1.313], mean_best_reward: --\n",
      "  6171/100000: episode: 235, duration: 0.033s, episode steps: 44, steps per second: 1326, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.113 [-0.377, 1.181], mean_best_reward: --\n",
      "  6195/100000: episode: 236, duration: 0.019s, episode steps: 24, steps per second: 1277, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.069 [-0.653, 1.315], mean_best_reward: --\n",
      "  6215/100000: episode: 237, duration: 0.016s, episode steps: 20, steps per second: 1229, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.049 [-2.178, 1.398], mean_best_reward: --\n",
      "  6275/100000: episode: 238, duration: 0.049s, episode steps: 60, steps per second: 1224, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.140 [-0.535, 1.253], mean_best_reward: --\n",
      "  6308/100000: episode: 239, duration: 0.032s, episode steps: 33, steps per second: 1036, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.092 [-0.618, 0.908], mean_best_reward: --\n",
      "  6341/100000: episode: 240, duration: 0.028s, episode steps: 33, steps per second: 1194, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.074 [-0.586, 0.940], mean_best_reward: --\n",
      "  6355/100000: episode: 241, duration: 0.014s, episode steps: 14, steps per second: 1000, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.113 [-1.177, 1.764], mean_best_reward: --\n",
      "  6412/100000: episode: 242, duration: 0.041s, episode steps: 57, steps per second: 1380, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.046 [-1.006, 0.847], mean_best_reward: --\n",
      "  6478/100000: episode: 243, duration: 0.045s, episode steps: 66, steps per second: 1458, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.223 [-1.406, 0.767], mean_best_reward: --\n",
      "  6495/100000: episode: 244, duration: 0.013s, episode steps: 17, steps per second: 1351, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.099 [-1.429, 0.753], mean_best_reward: --\n",
      "  6536/100000: episode: 245, duration: 0.036s, episode steps: 41, steps per second: 1131, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.027 [-0.946, 0.650], mean_best_reward: --\n",
      "  6597/100000: episode: 246, duration: 0.047s, episode steps: 61, steps per second: 1293, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.083 [-1.067, 1.555], mean_best_reward: --\n",
      "  6616/100000: episode: 247, duration: 0.015s, episode steps: 19, steps per second: 1270, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.081 [-0.758, 1.228], mean_best_reward: --\n",
      "  6684/100000: episode: 248, duration: 0.058s, episode steps: 68, steps per second: 1169, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.133 [-1.026, 1.553], mean_best_reward: --\n",
      "  6695/100000: episode: 249, duration: 0.010s, episode steps: 11, steps per second: 1121, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.138 [-1.606, 0.943], mean_best_reward: --\n",
      "  6749/100000: episode: 250, duration: 0.030s, episode steps: 54, steps per second: 1799, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.066 [-0.943, 1.194], mean_best_reward: --\n",
      "  6782/100000: episode: 251, duration: 0.019s, episode steps: 33, steps per second: 1712, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.606 [0.000, 1.000], mean observation: 0.072 [-1.723, 1.544], mean_best_reward: 64.500000\n",
      "  6865/100000: episode: 252, duration: 0.065s, episode steps: 83, steps per second: 1284, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.026 [-0.933, 1.186], mean_best_reward: --\n",
      "  6882/100000: episode: 253, duration: 0.016s, episode steps: 17, steps per second: 1041, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.115 [-0.939, 1.739], mean_best_reward: --\n",
      "  6918/100000: episode: 254, duration: 0.032s, episode steps: 36, steps per second: 1123, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.151 [-0.482, 0.926], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6956/100000: episode: 255, duration: 0.037s, episode steps: 38, steps per second: 1017, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.124 [-0.452, 0.821], mean_best_reward: --\n",
      "  7000/100000: episode: 256, duration: 0.029s, episode steps: 44, steps per second: 1531, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.014 [-1.402, 1.181], mean_best_reward: --\n",
      "  7068/100000: episode: 257, duration: 0.056s, episode steps: 68, steps per second: 1215, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.141 [-1.222, 1.548], mean_best_reward: --\n",
      "  7105/100000: episode: 258, duration: 0.029s, episode steps: 37, steps per second: 1258, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.089 [-0.880, 0.604], mean_best_reward: --\n",
      "  7139/100000: episode: 259, duration: 0.030s, episode steps: 34, steps per second: 1151, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.049 [-0.603, 1.062], mean_best_reward: --\n",
      "  7157/100000: episode: 260, duration: 0.016s, episode steps: 18, steps per second: 1092, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-1.233, 0.766], mean_best_reward: --\n",
      "  7173/100000: episode: 261, duration: 0.026s, episode steps: 16, steps per second: 609, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.102 [-1.006, 1.829], mean_best_reward: --\n",
      "  7225/100000: episode: 262, duration: 0.036s, episode steps: 52, steps per second: 1443, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.003 [-1.184, 1.456], mean_best_reward: --\n",
      "  7270/100000: episode: 263, duration: 0.025s, episode steps: 45, steps per second: 1778, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.094 [-0.791, 1.555], mean_best_reward: --\n",
      "  7317/100000: episode: 264, duration: 0.033s, episode steps: 47, steps per second: 1417, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.004 [-0.995, 1.438], mean_best_reward: --\n",
      "  7340/100000: episode: 265, duration: 0.018s, episode steps: 23, steps per second: 1247, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.082 [-0.643, 1.108], mean_best_reward: --\n",
      "  7381/100000: episode: 266, duration: 0.037s, episode steps: 41, steps per second: 1110, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.164 [-1.237, 0.787], mean_best_reward: --\n",
      "  7398/100000: episode: 267, duration: 0.014s, episode steps: 17, steps per second: 1218, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.049 [-0.994, 1.499], mean_best_reward: --\n",
      "  7440/100000: episode: 268, duration: 0.037s, episode steps: 42, steps per second: 1132, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.606, 1.289], mean_best_reward: --\n",
      "  7459/100000: episode: 269, duration: 0.020s, episode steps: 19, steps per second: 949, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.111 [-1.027, 0.565], mean_best_reward: --\n",
      "  7480/100000: episode: 270, duration: 0.020s, episode steps: 21, steps per second: 1075, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.036 [-1.700, 1.168], mean_best_reward: --\n",
      "  7513/100000: episode: 271, duration: 0.023s, episode steps: 33, steps per second: 1440, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.069 [-1.722, 0.799], mean_best_reward: --\n",
      "  7573/100000: episode: 272, duration: 0.039s, episode steps: 60, steps per second: 1541, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.128 [-1.645, 1.915], mean_best_reward: --\n",
      "  7661/100000: episode: 273, duration: 0.070s, episode steps: 88, steps per second: 1249, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.134 [-1.266, 1.475], mean_best_reward: --\n",
      "  7685/100000: episode: 274, duration: 0.020s, episode steps: 24, steps per second: 1227, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.099 [-0.964, 0.559], mean_best_reward: --\n",
      "  7699/100000: episode: 275, duration: 0.010s, episode steps: 14, steps per second: 1453, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.122 [-0.770, 1.609], mean_best_reward: --\n",
      "  7716/100000: episode: 276, duration: 0.012s, episode steps: 17, steps per second: 1473, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.083 [-0.818, 1.337], mean_best_reward: --\n",
      "  7763/100000: episode: 277, duration: 0.038s, episode steps: 47, steps per second: 1230, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.037 [-1.607, 1.125], mean_best_reward: --\n",
      "  7776/100000: episode: 278, duration: 0.009s, episode steps: 13, steps per second: 1390, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.105 [-0.742, 1.278], mean_best_reward: --\n",
      "  7792/100000: episode: 279, duration: 0.013s, episode steps: 16, steps per second: 1213, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.069 [-1.619, 1.125], mean_best_reward: --\n",
      "  7819/100000: episode: 280, duration: 0.021s, episode steps: 27, steps per second: 1316, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.022 [-1.420, 0.989], mean_best_reward: --\n",
      "  7846/100000: episode: 281, duration: 0.017s, episode steps: 27, steps per second: 1560, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.044 [-0.730, 1.079], mean_best_reward: --\n",
      "  7874/100000: episode: 282, duration: 0.021s, episode steps: 28, steps per second: 1343, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.117 [-1.429, 0.956], mean_best_reward: --\n",
      "  7898/100000: episode: 283, duration: 0.018s, episode steps: 24, steps per second: 1326, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.088 [-0.877, 0.537], mean_best_reward: --\n",
      "  7982/100000: episode: 284, duration: 0.066s, episode steps: 84, steps per second: 1271, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.151 [-1.510, 0.908], mean_best_reward: --\n",
      "  7999/100000: episode: 285, duration: 0.011s, episode steps: 17, steps per second: 1532, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.080 [-0.967, 1.646], mean_best_reward: --\n",
      "  8034/100000: episode: 286, duration: 0.028s, episode steps: 35, steps per second: 1270, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.027 [-1.497, 1.121], mean_best_reward: --\n",
      "  8142/100000: episode: 287, duration: 0.078s, episode steps: 108, steps per second: 1392, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.197 [-1.979, 0.859], mean_best_reward: --\n",
      "  8182/100000: episode: 288, duration: 0.028s, episode steps: 40, steps per second: 1409, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.098 [-0.954, 1.936], mean_best_reward: --\n",
      "  8201/100000: episode: 289, duration: 0.013s, episode steps: 19, steps per second: 1486, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.125 [-0.950, 1.526], mean_best_reward: --\n",
      "  8217/100000: episode: 290, duration: 0.012s, episode steps: 16, steps per second: 1304, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.091 [-0.999, 1.734], mean_best_reward: --\n",
      "  8249/100000: episode: 291, duration: 0.040s, episode steps: 32, steps per second: 810, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.406 [0.000, 1.000], mean observation: 0.059 [-1.136, 1.960], mean_best_reward: --\n",
      "  8261/100000: episode: 292, duration: 0.009s, episode steps: 12, steps per second: 1313, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.121 [-2.245, 1.353], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8314/100000: episode: 293, duration: 0.061s, episode steps: 53, steps per second: 867, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.180 [-0.940, 1.350], mean_best_reward: --\n",
      "  8349/100000: episode: 294, duration: 0.038s, episode steps: 35, steps per second: 916, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.057 [-0.815, 1.562], mean_best_reward: --\n",
      "  8362/100000: episode: 295, duration: 0.015s, episode steps: 13, steps per second: 854, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.119 [-1.208, 0.734], mean_best_reward: --\n",
      "  8374/100000: episode: 296, duration: 0.011s, episode steps: 12, steps per second: 1061, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.104 [-0.996, 1.666], mean_best_reward: --\n",
      "  8393/100000: episode: 297, duration: 0.028s, episode steps: 19, steps per second: 678, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.094 [-0.602, 1.118], mean_best_reward: --\n",
      "  8404/100000: episode: 298, duration: 0.017s, episode steps: 11, steps per second: 633, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.080 [-1.877, 1.217], mean_best_reward: --\n",
      "  8517/100000: episode: 299, duration: 0.066s, episode steps: 113, steps per second: 1709, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.106 [-1.664, 1.394], mean_best_reward: --\n",
      "  8534/100000: episode: 300, duration: 0.011s, episode steps: 17, steps per second: 1544, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.090 [-1.484, 1.015], mean_best_reward: --\n",
      "  8617/100000: episode: 301, duration: 0.064s, episode steps: 83, steps per second: 1291, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.111 [-1.844, 0.773], mean_best_reward: 65.500000\n",
      "  8655/100000: episode: 302, duration: 0.027s, episode steps: 38, steps per second: 1418, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.115 [-0.875, 0.571], mean_best_reward: --\n",
      "  8703/100000: episode: 303, duration: 0.032s, episode steps: 48, steps per second: 1479, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.108 [-1.746, 0.576], mean_best_reward: --\n",
      "  8740/100000: episode: 304, duration: 0.028s, episode steps: 37, steps per second: 1299, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.111 [-0.550, 1.004], mean_best_reward: --\n",
      "  8772/100000: episode: 305, duration: 0.033s, episode steps: 32, steps per second: 960, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-1.117, 0.759], mean_best_reward: --\n",
      "  8793/100000: episode: 306, duration: 0.026s, episode steps: 21, steps per second: 816, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.128 [-0.746, 1.206], mean_best_reward: --\n",
      "  8815/100000: episode: 307, duration: 0.017s, episode steps: 22, steps per second: 1267, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.068 [-1.413, 1.022], mean_best_reward: --\n",
      "  8840/100000: episode: 308, duration: 0.023s, episode steps: 25, steps per second: 1089, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.036 [-1.189, 1.768], mean_best_reward: --\n",
      "  8863/100000: episode: 309, duration: 0.025s, episode steps: 23, steps per second: 907, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.084 [-1.143, 1.838], mean_best_reward: --\n",
      "  8876/100000: episode: 310, duration: 0.026s, episode steps: 13, steps per second: 493, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.113 [-1.606, 0.971], mean_best_reward: --\n",
      "  8891/100000: episode: 311, duration: 0.021s, episode steps: 15, steps per second: 712, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.099 [-0.801, 1.555], mean_best_reward: --\n",
      "  8918/100000: episode: 312, duration: 0.032s, episode steps: 27, steps per second: 843, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.103 [-0.735, 1.325], mean_best_reward: --\n",
      "  8937/100000: episode: 313, duration: 0.029s, episode steps: 19, steps per second: 653, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.075 [-1.380, 2.341], mean_best_reward: --\n",
      "  8950/100000: episode: 314, duration: 0.008s, episode steps: 13, steps per second: 1565, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.105 [-1.741, 1.021], mean_best_reward: --\n",
      "  8975/100000: episode: 315, duration: 0.016s, episode steps: 25, steps per second: 1535, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.014 [-1.423, 1.002], mean_best_reward: --\n",
      "  8990/100000: episode: 316, duration: 0.009s, episode steps: 15, steps per second: 1685, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.076 [-1.725, 1.028], mean_best_reward: --\n",
      "  9031/100000: episode: 317, duration: 0.022s, episode steps: 41, steps per second: 1894, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.150 [-1.162, 0.641], mean_best_reward: --\n",
      "  9042/100000: episode: 318, duration: 0.007s, episode steps: 11, steps per second: 1597, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.123 [-2.228, 1.376], mean_best_reward: --\n",
      "  9056/100000: episode: 319, duration: 0.008s, episode steps: 14, steps per second: 1704, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.098 [-0.749, 1.377], mean_best_reward: --\n",
      "  9127/100000: episode: 320, duration: 0.046s, episode steps: 71, steps per second: 1546, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.040 [-0.957, 1.386], mean_best_reward: --\n",
      "  9164/100000: episode: 321, duration: 0.020s, episode steps: 37, steps per second: 1825, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.045 [-0.755, 1.416], mean_best_reward: --\n",
      "  9176/100000: episode: 322, duration: 0.008s, episode steps: 12, steps per second: 1516, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.515, 1.017], mean_best_reward: --\n",
      "  9205/100000: episode: 323, duration: 0.023s, episode steps: 29, steps per second: 1282, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: 0.069 [-1.376, 2.437], mean_best_reward: --\n",
      "  9257/100000: episode: 324, duration: 0.034s, episode steps: 52, steps per second: 1516, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.002 [-0.991, 1.475], mean_best_reward: --\n",
      "  9329/100000: episode: 325, duration: 0.058s, episode steps: 72, steps per second: 1235, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.079 [-0.807, 1.194], mean_best_reward: --\n",
      "  9350/100000: episode: 326, duration: 0.018s, episode steps: 21, steps per second: 1141, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.064 [-1.127, 1.886], mean_best_reward: --\n",
      "  9362/100000: episode: 327, duration: 0.011s, episode steps: 12, steps per second: 1111, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.094 [-2.013, 1.390], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9403/100000: episode: 328, duration: 0.030s, episode steps: 41, steps per second: 1388, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.119 [-0.876, 0.434], mean_best_reward: --\n",
      "  9415/100000: episode: 329, duration: 0.011s, episode steps: 12, steps per second: 1085, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.113 [-0.968, 1.510], mean_best_reward: --\n",
      "  9468/100000: episode: 330, duration: 0.032s, episode steps: 53, steps per second: 1652, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.179 [-1.143, 0.540], mean_best_reward: --\n",
      "  9499/100000: episode: 331, duration: 0.021s, episode steps: 31, steps per second: 1472, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.040 [-1.729, 1.129], mean_best_reward: --\n",
      "  9518/100000: episode: 332, duration: 0.013s, episode steps: 19, steps per second: 1409, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.076 [-0.818, 1.448], mean_best_reward: --\n",
      "  9547/100000: episode: 333, duration: 0.027s, episode steps: 29, steps per second: 1074, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.069 [-0.595, 1.001], mean_best_reward: --\n",
      "  9614/100000: episode: 334, duration: 0.051s, episode steps: 67, steps per second: 1310, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.003 [-1.489, 1.307], mean_best_reward: --\n",
      "  9638/100000: episode: 335, duration: 0.017s, episode steps: 24, steps per second: 1422, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.016 [-1.157, 1.634], mean_best_reward: --\n",
      "  9647/100000: episode: 336, duration: 0.009s, episode steps: 9, steps per second: 956, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-1.222, 1.840], mean_best_reward: --\n",
      "  9679/100000: episode: 337, duration: 0.031s, episode steps: 32, steps per second: 1030, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.111 [-0.471, 1.547], mean_best_reward: --\n",
      "  9703/100000: episode: 338, duration: 0.017s, episode steps: 24, steps per second: 1387, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.113 [-0.810, 1.826], mean_best_reward: --\n",
      "  9718/100000: episode: 339, duration: 0.014s, episode steps: 15, steps per second: 1089, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.107 [-0.975, 1.503], mean_best_reward: --\n",
      "  9738/100000: episode: 340, duration: 0.018s, episode steps: 20, steps per second: 1100, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.112 [-0.952, 1.849], mean_best_reward: --\n",
      "  9756/100000: episode: 341, duration: 0.017s, episode steps: 18, steps per second: 1065, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.085 [-0.593, 1.298], mean_best_reward: --\n",
      "  9790/100000: episode: 342, duration: 0.027s, episode steps: 34, steps per second: 1239, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.051 [-1.149, 0.765], mean_best_reward: --\n",
      "  9823/100000: episode: 343, duration: 0.029s, episode steps: 33, steps per second: 1156, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: 0.020 [-1.826, 1.510], mean_best_reward: --\n",
      "  9862/100000: episode: 344, duration: 0.024s, episode steps: 39, steps per second: 1612, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.065 [-0.841, 1.716], mean_best_reward: --\n",
      "  9899/100000: episode: 345, duration: 0.025s, episode steps: 37, steps per second: 1479, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.057 [-1.121, 0.609], mean_best_reward: --\n",
      "  9958/100000: episode: 346, duration: 0.048s, episode steps: 59, steps per second: 1226, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.203 [-1.276, 0.961], mean_best_reward: --\n",
      "  9974/100000: episode: 347, duration: 0.011s, episode steps: 16, steps per second: 1392, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.059 [-1.754, 1.189], mean_best_reward: --\n",
      "  9991/100000: episode: 348, duration: 0.013s, episode steps: 17, steps per second: 1289, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.078 [-1.324, 2.227], mean_best_reward: --\n",
      " 10009/100000: episode: 349, duration: 0.014s, episode steps: 18, steps per second: 1307, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.090 [-0.950, 1.699], mean_best_reward: --\n",
      " 10045/100000: episode: 350, duration: 0.024s, episode steps: 36, steps per second: 1481, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.166 [-0.980, 0.552], mean_best_reward: --\n",
      " 10087/100000: episode: 351, duration: 0.032s, episode steps: 42, steps per second: 1328, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.060 [-0.989, 1.550], mean_best_reward: 88.500000\n",
      " 10104/100000: episode: 352, duration: 0.015s, episode steps: 17, steps per second: 1110, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.056 [-1.167, 1.619], mean_best_reward: --\n",
      " 10119/100000: episode: 353, duration: 0.017s, episode steps: 15, steps per second: 905, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.095 [-1.009, 1.707], mean_best_reward: --\n",
      " 10130/100000: episode: 354, duration: 0.008s, episode steps: 11, steps per second: 1397, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.119 [-1.134, 1.968], mean_best_reward: --\n",
      " 10156/100000: episode: 355, duration: 0.021s, episode steps: 26, steps per second: 1258, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.031 [-1.196, 1.817], mean_best_reward: --\n",
      " 10177/100000: episode: 356, duration: 0.021s, episode steps: 21, steps per second: 989, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.115 [-1.245, 0.436], mean_best_reward: --\n",
      " 10236/100000: episode: 357, duration: 0.050s, episode steps: 59, steps per second: 1169, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.073 [-1.183, 0.455], mean_best_reward: --\n",
      " 10257/100000: episode: 358, duration: 0.015s, episode steps: 21, steps per second: 1408, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.103 [-1.561, 0.758], mean_best_reward: --\n",
      " 10276/100000: episode: 359, duration: 0.015s, episode steps: 19, steps per second: 1265, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.107 [-1.354, 2.433], mean_best_reward: --\n",
      " 10341/100000: episode: 360, duration: 0.040s, episode steps: 65, steps per second: 1636, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.056 [-1.048, 1.968], mean_best_reward: --\n",
      " 10406/100000: episode: 361, duration: 0.044s, episode steps: 65, steps per second: 1480, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.017 [-0.868, 0.595], mean_best_reward: --\n",
      " 10458/100000: episode: 362, duration: 0.042s, episode steps: 52, steps per second: 1241, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.151 [-1.427, 0.606], mean_best_reward: --\n",
      " 10469/100000: episode: 363, duration: 0.011s, episode steps: 11, steps per second: 1016, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.134 [-1.010, 1.716], mean_best_reward: --\n",
      " 10505/100000: episode: 364, duration: 0.025s, episode steps: 36, steps per second: 1446, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.019 [-1.377, 1.010], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10560/100000: episode: 365, duration: 0.043s, episode steps: 55, steps per second: 1294, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.061 [-1.348, 0.672], mean_best_reward: --\n",
      " 10597/100000: episode: 366, duration: 0.026s, episode steps: 37, steps per second: 1445, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.047 [-0.770, 1.197], mean_best_reward: --\n",
      " 10655/100000: episode: 367, duration: 0.039s, episode steps: 58, steps per second: 1500, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.569 [0.000, 1.000], mean observation: 0.082 [-2.298, 1.602], mean_best_reward: --\n",
      " 10683/100000: episode: 368, duration: 0.022s, episode steps: 28, steps per second: 1245, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.057 [-1.427, 0.790], mean_best_reward: --\n",
      " 10696/100000: episode: 369, duration: 0.011s, episode steps: 13, steps per second: 1148, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.117 [-0.952, 1.776], mean_best_reward: --\n",
      " 10721/100000: episode: 370, duration: 0.017s, episode steps: 25, steps per second: 1455, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.091 [-1.219, 0.566], mean_best_reward: --\n",
      " 10741/100000: episode: 371, duration: 0.016s, episode steps: 20, steps per second: 1239, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.058 [-0.969, 1.563], mean_best_reward: --\n",
      " 10777/100000: episode: 372, duration: 0.027s, episode steps: 36, steps per second: 1317, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.010 [-0.985, 1.416], mean_best_reward: --\n",
      " 10808/100000: episode: 373, duration: 0.020s, episode steps: 31, steps per second: 1568, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.058 [-0.762, 1.392], mean_best_reward: --\n",
      " 10868/100000: episode: 374, duration: 0.054s, episode steps: 60, steps per second: 1109, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.778, 1.042], mean_best_reward: --\n",
      " 10892/100000: episode: 375, duration: 0.027s, episode steps: 24, steps per second: 896, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.063 [-0.758, 1.399], mean_best_reward: --\n",
      " 10924/100000: episode: 376, duration: 0.037s, episode steps: 32, steps per second: 875, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.014 [-1.210, 1.612], mean_best_reward: --\n",
      " 10936/100000: episode: 377, duration: 0.018s, episode steps: 12, steps per second: 679, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-0.836, 1.581], mean_best_reward: --\n",
      " 10960/100000: episode: 378, duration: 0.033s, episode steps: 24, steps per second: 721, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.575, 1.214], mean_best_reward: --\n",
      " 11010/100000: episode: 379, duration: 0.074s, episode steps: 50, steps per second: 677, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.005 [-0.961, 1.276], mean_best_reward: --\n",
      " 11028/100000: episode: 380, duration: 0.025s, episode steps: 18, steps per second: 716, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.102 [-0.596, 1.103], mean_best_reward: --\n",
      " 11048/100000: episode: 381, duration: 0.019s, episode steps: 20, steps per second: 1041, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.080 [-1.612, 0.935], mean_best_reward: --\n",
      " 11109/100000: episode: 382, duration: 0.056s, episode steps: 61, steps per second: 1091, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.124 [-1.177, 0.599], mean_best_reward: --\n",
      " 11161/100000: episode: 383, duration: 0.046s, episode steps: 52, steps per second: 1141, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.115 [-1.831, 1.069], mean_best_reward: --\n",
      " 11197/100000: episode: 384, duration: 0.035s, episode steps: 36, steps per second: 1022, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-1.033, 1.662], mean_best_reward: --\n",
      " 11236/100000: episode: 385, duration: 0.030s, episode steps: 39, steps per second: 1288, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.124 [-0.900, 0.391], mean_best_reward: --\n",
      " 11273/100000: episode: 386, duration: 0.026s, episode steps: 37, steps per second: 1430, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.084 [-0.552, 1.017], mean_best_reward: --\n",
      " 11341/100000: episode: 387, duration: 0.063s, episode steps: 68, steps per second: 1086, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.294 [-0.842, 1.671], mean_best_reward: --\n",
      " 11386/100000: episode: 388, duration: 0.087s, episode steps: 45, steps per second: 516, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.114 [-0.679, 0.945], mean_best_reward: --\n",
      " 11428/100000: episode: 389, duration: 0.052s, episode steps: 42, steps per second: 810, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-1.437, 0.782], mean_best_reward: --\n",
      " 11496/100000: episode: 390, duration: 0.051s, episode steps: 68, steps per second: 1325, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.051 [-1.215, 1.972], mean_best_reward: --\n",
      " 11536/100000: episode: 391, duration: 0.042s, episode steps: 40, steps per second: 954, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.916, 1.315], mean_best_reward: --\n",
      " 11548/100000: episode: 392, duration: 0.017s, episode steps: 12, steps per second: 705, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.107 [-1.841, 1.207], mean_best_reward: --\n",
      " 11593/100000: episode: 393, duration: 0.048s, episode steps: 45, steps per second: 946, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.578 [0.000, 1.000], mean observation: 0.211 [-0.757, 1.301], mean_best_reward: --\n",
      " 11650/100000: episode: 394, duration: 0.079s, episode steps: 57, steps per second: 722, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.035 [-0.824, 1.381], mean_best_reward: --\n",
      " 11678/100000: episode: 395, duration: 0.035s, episode steps: 28, steps per second: 808, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: -0.016 [-1.365, 1.737], mean_best_reward: --\n",
      " 11708/100000: episode: 396, duration: 0.024s, episode steps: 30, steps per second: 1239, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.102 [-0.413, 0.757], mean_best_reward: --\n",
      " 11767/100000: episode: 397, duration: 0.034s, episode steps: 59, steps per second: 1733, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.148 [-0.785, 1.505], mean_best_reward: --\n",
      " 11811/100000: episode: 398, duration: 0.027s, episode steps: 44, steps per second: 1615, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.040 [-0.976, 1.793], mean_best_reward: --\n",
      " 11850/100000: episode: 399, duration: 0.034s, episode steps: 39, steps per second: 1153, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.114 [-0.840, 1.206], mean_best_reward: --\n",
      " 11885/100000: episode: 400, duration: 0.034s, episode steps: 35, steps per second: 1035, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.063 [-0.585, 1.281], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11905/100000: episode: 401, duration: 0.021s, episode steps: 20, steps per second: 955, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.080 [-0.640, 1.112], mean_best_reward: 86.000000\n",
      " 11921/100000: episode: 402, duration: 0.012s, episode steps: 16, steps per second: 1330, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.067 [-1.013, 1.575], mean_best_reward: --\n",
      " 11970/100000: episode: 403, duration: 0.042s, episode steps: 49, steps per second: 1178, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: -0.066 [-2.043, 1.012], mean_best_reward: --\n",
      " 11993/100000: episode: 404, duration: 0.021s, episode steps: 23, steps per second: 1083, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.071 [-1.391, 0.765], mean_best_reward: --\n",
      " 12084/100000: episode: 405, duration: 0.090s, episode steps: 91, steps per second: 1007, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.116 [-1.326, 0.803], mean_best_reward: --\n",
      " 12110/100000: episode: 406, duration: 0.024s, episode steps: 26, steps per second: 1073, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.116 [-0.962, 0.589], mean_best_reward: --\n",
      " 12132/100000: episode: 407, duration: 0.023s, episode steps: 22, steps per second: 965, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.112 [-1.245, 0.559], mean_best_reward: --\n",
      " 12167/100000: episode: 408, duration: 0.028s, episode steps: 35, steps per second: 1242, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.005 [-1.173, 1.555], mean_best_reward: --\n",
      " 12183/100000: episode: 409, duration: 0.020s, episode steps: 16, steps per second: 784, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.619, 1.069], mean_best_reward: --\n",
      " 12205/100000: episode: 410, duration: 0.026s, episode steps: 22, steps per second: 860, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.044 [-0.620, 1.030], mean_best_reward: --\n",
      " 12257/100000: episode: 411, duration: 0.049s, episode steps: 52, steps per second: 1068, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.086 [-1.644, 0.880], mean_best_reward: --\n",
      " 12273/100000: episode: 412, duration: 0.017s, episode steps: 16, steps per second: 934, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.569, 1.118], mean_best_reward: --\n",
      " 12350/100000: episode: 413, duration: 0.082s, episode steps: 77, steps per second: 942, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: 0.090 [-1.180, 2.202], mean_best_reward: --\n",
      " 12406/100000: episode: 414, duration: 0.047s, episode steps: 56, steps per second: 1203, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: 0.035 [-1.177, 1.435], mean_best_reward: --\n",
      " 12435/100000: episode: 415, duration: 0.027s, episode steps: 29, steps per second: 1088, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.106 [-1.452, 0.961], mean_best_reward: --\n",
      " 12470/100000: episode: 416, duration: 0.039s, episode steps: 35, steps per second: 907, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.024 [-1.321, 1.526], mean_best_reward: --\n",
      " 12490/100000: episode: 417, duration: 0.025s, episode steps: 20, steps per second: 810, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.045 [-1.595, 1.000], mean_best_reward: --\n",
      " 12500/100000: episode: 418, duration: 0.011s, episode steps: 10, steps per second: 943, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.134 [-0.961, 1.653], mean_best_reward: --\n",
      " 12509/100000: episode: 419, duration: 0.015s, episode steps: 9, steps per second: 610, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.154 [-1.128, 1.877], mean_best_reward: --\n",
      " 12524/100000: episode: 420, duration: 0.012s, episode steps: 15, steps per second: 1239, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.104 [-1.429, 0.760], mean_best_reward: --\n",
      " 12622/100000: episode: 421, duration: 0.084s, episode steps: 98, steps per second: 1166, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.077 [-1.238, 1.368], mean_best_reward: --\n",
      " 12671/100000: episode: 422, duration: 0.030s, episode steps: 49, steps per second: 1630, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.117 [-1.266, 0.825], mean_best_reward: --\n",
      " 12733/100000: episode: 423, duration: 0.053s, episode steps: 62, steps per second: 1175, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.009 [-0.967, 1.259], mean_best_reward: --\n",
      " 12772/100000: episode: 424, duration: 0.044s, episode steps: 39, steps per second: 877, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.135 [-0.590, 0.892], mean_best_reward: --\n",
      " 12815/100000: episode: 425, duration: 0.044s, episode steps: 43, steps per second: 985, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.074 [-0.780, 1.216], mean_best_reward: --\n",
      " 12841/100000: episode: 426, duration: 0.023s, episode steps: 26, steps per second: 1140, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.053 [-0.602, 0.925], mean_best_reward: --\n",
      " 12877/100000: episode: 427, duration: 0.042s, episode steps: 36, steps per second: 859, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.076 [-0.651, 1.459], mean_best_reward: --\n",
      " 12904/100000: episode: 428, duration: 0.026s, episode steps: 27, steps per second: 1049, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.150 [-0.584, 0.992], mean_best_reward: --\n",
      " 12923/100000: episode: 429, duration: 0.013s, episode steps: 19, steps per second: 1442, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.125 [-0.573, 0.998], mean_best_reward: --\n",
      " 12944/100000: episode: 430, duration: 0.013s, episode steps: 21, steps per second: 1648, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.110 [-1.375, 0.602], mean_best_reward: --\n",
      " 12962/100000: episode: 431, duration: 0.012s, episode steps: 18, steps per second: 1532, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.087 [-1.160, 0.771], mean_best_reward: --\n",
      " 13024/100000: episode: 432, duration: 0.036s, episode steps: 62, steps per second: 1707, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.177 [-0.886, 1.677], mean_best_reward: --\n",
      " 13052/100000: episode: 433, duration: 0.023s, episode steps: 28, steps per second: 1219, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.050 [-0.828, 1.544], mean_best_reward: --\n",
      " 13081/100000: episode: 434, duration: 0.025s, episode steps: 29, steps per second: 1161, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.059 [-1.420, 0.757], mean_best_reward: --\n",
      " 13112/100000: episode: 435, duration: 0.022s, episode steps: 31, steps per second: 1436, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.124 [-0.565, 0.977], mean_best_reward: --\n",
      " 13146/100000: episode: 436, duration: 0.023s, episode steps: 34, steps per second: 1450, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.118 [-1.022, 0.596], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13193/100000: episode: 437, duration: 0.033s, episode steps: 47, steps per second: 1445, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.024 [-1.540, 1.947], mean_best_reward: --\n",
      " 13261/100000: episode: 438, duration: 0.042s, episode steps: 68, steps per second: 1600, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.718, 1.246], mean_best_reward: --\n",
      " 13272/100000: episode: 439, duration: 0.009s, episode steps: 11, steps per second: 1174, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.111 [-1.599, 1.014], mean_best_reward: --\n",
      " 13336/100000: episode: 440, duration: 0.056s, episode steps: 64, steps per second: 1134, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.036 [-0.990, 1.635], mean_best_reward: --\n",
      " 13415/100000: episode: 441, duration: 0.060s, episode steps: 79, steps per second: 1314, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.025 [-1.629, 1.213], mean_best_reward: --\n",
      " 13458/100000: episode: 442, duration: 0.036s, episode steps: 43, steps per second: 1206, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.137 [-0.623, 0.937], mean_best_reward: --\n",
      " 13478/100000: episode: 443, duration: 0.017s, episode steps: 20, steps per second: 1160, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.079 [-1.852, 1.169], mean_best_reward: --\n",
      " 13511/100000: episode: 444, duration: 0.024s, episode steps: 33, steps per second: 1369, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.028 [-1.724, 2.672], mean_best_reward: --\n",
      " 13545/100000: episode: 445, duration: 0.018s, episode steps: 34, steps per second: 1870, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.113 [-0.843, 2.095], mean_best_reward: --\n",
      " 13567/100000: episode: 446, duration: 0.012s, episode steps: 22, steps per second: 1778, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.011, 0.622], mean_best_reward: --\n",
      " 13585/100000: episode: 447, duration: 0.010s, episode steps: 18, steps per second: 1744, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.053 [-0.955, 1.490], mean_best_reward: --\n",
      " 13615/100000: episode: 448, duration: 0.019s, episode steps: 30, steps per second: 1560, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.643, 1.298], mean_best_reward: --\n",
      " 13630/100000: episode: 449, duration: 0.014s, episode steps: 15, steps per second: 1106, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.063 [-1.743, 1.032], mean_best_reward: --\n",
      " 13669/100000: episode: 450, duration: 0.028s, episode steps: 39, steps per second: 1405, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.009 [-1.135, 1.550], mean_best_reward: --\n",
      " 13718/100000: episode: 451, duration: 0.036s, episode steps: 49, steps per second: 1370, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.046 [-1.406, 0.815], mean_best_reward: 75.000000\n",
      " 13744/100000: episode: 452, duration: 0.024s, episode steps: 26, steps per second: 1099, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.048 [-1.409, 0.948], mean_best_reward: --\n",
      " 13772/100000: episode: 453, duration: 0.021s, episode steps: 28, steps per second: 1322, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.035 [-1.160, 0.812], mean_best_reward: --\n",
      " 13791/100000: episode: 454, duration: 0.012s, episode steps: 19, steps per second: 1619, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.078 [-0.654, 1.440], mean_best_reward: --\n",
      " 13812/100000: episode: 455, duration: 0.013s, episode steps: 21, steps per second: 1613, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.111 [-0.591, 1.008], mean_best_reward: --\n",
      " 13839/100000: episode: 456, duration: 0.018s, episode steps: 27, steps per second: 1533, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.073 [-1.004, 1.814], mean_best_reward: --\n",
      " 13884/100000: episode: 457, duration: 0.030s, episode steps: 45, steps per second: 1482, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.006 [-1.107, 0.746], mean_best_reward: --\n",
      " 13919/100000: episode: 458, duration: 0.023s, episode steps: 35, steps per second: 1541, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.109 [-0.669, 1.957], mean_best_reward: --\n",
      " 13950/100000: episode: 459, duration: 0.026s, episode steps: 31, steps per second: 1179, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.138 [-0.938, 0.589], mean_best_reward: --\n",
      " 14025/100000: episode: 460, duration: 0.064s, episode steps: 75, steps per second: 1172, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.079 [-1.376, 1.546], mean_best_reward: --\n",
      " 14034/100000: episode: 461, duration: 0.009s, episode steps: 9, steps per second: 987, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.160 [-1.785, 0.965], mean_best_reward: --\n",
      " 14060/100000: episode: 462, duration: 0.015s, episode steps: 26, steps per second: 1694, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.022 [-1.234, 0.802], mean_best_reward: --\n",
      " 14101/100000: episode: 463, duration: 0.031s, episode steps: 41, steps per second: 1334, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.018 [-1.009, 1.553], mean_best_reward: --\n",
      " 14121/100000: episode: 464, duration: 0.012s, episode steps: 20, steps per second: 1633, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.102 [-0.546, 1.027], mean_best_reward: --\n",
      " 14156/100000: episode: 465, duration: 0.021s, episode steps: 35, steps per second: 1692, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.069 [-0.885, 0.611], mean_best_reward: --\n",
      " 14172/100000: episode: 466, duration: 0.013s, episode steps: 16, steps per second: 1262, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.077 [-1.789, 1.017], mean_best_reward: --\n",
      " 14211/100000: episode: 467, duration: 0.043s, episode steps: 39, steps per second: 915, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.069 [-0.811, 1.225], mean_best_reward: --\n",
      " 14231/100000: episode: 468, duration: 0.020s, episode steps: 20, steps per second: 999, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.080 [-1.253, 0.798], mean_best_reward: --\n",
      " 14288/100000: episode: 469, duration: 0.050s, episode steps: 57, steps per second: 1147, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.107 [-1.314, 1.029], mean_best_reward: --\n",
      " 14326/100000: episode: 470, duration: 0.034s, episode steps: 38, steps per second: 1105, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.091 [-1.902, 0.958], mean_best_reward: --\n",
      " 14338/100000: episode: 471, duration: 0.013s, episode steps: 12, steps per second: 946, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.127 [-0.788, 1.369], mean_best_reward: --\n",
      " 14370/100000: episode: 472, duration: 0.030s, episode steps: 32, steps per second: 1053, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.050 [-0.937, 1.586], mean_best_reward: --\n",
      " 14422/100000: episode: 473, duration: 0.051s, episode steps: 52, steps per second: 1030, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.116 [-0.402, 0.894], mean_best_reward: --\n",
      " 14449/100000: episode: 474, duration: 0.027s, episode steps: 27, steps per second: 986, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.078 [-0.576, 1.124], mean_best_reward: --\n",
      " 14464/100000: episode: 475, duration: 0.016s, episode steps: 15, steps per second: 964, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.082 [-1.006, 1.470], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14529/100000: episode: 476, duration: 0.055s, episode steps: 65, steps per second: 1176, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.057 [-0.835, 0.830], mean_best_reward: --\n",
      " 14616/100000: episode: 477, duration: 0.098s, episode steps: 87, steps per second: 886, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.039 [-1.276, 1.214], mean_best_reward: --\n",
      " 14639/100000: episode: 478, duration: 0.022s, episode steps: 23, steps per second: 1027, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.060 [-0.789, 1.226], mean_best_reward: --\n",
      " 14672/100000: episode: 479, duration: 0.041s, episode steps: 33, steps per second: 800, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.115 [-0.369, 0.767], mean_best_reward: --\n",
      " 14691/100000: episode: 480, duration: 0.029s, episode steps: 19, steps per second: 647, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.045 [-1.631, 1.031], mean_best_reward: --\n",
      " 14708/100000: episode: 481, duration: 0.032s, episode steps: 17, steps per second: 525, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.100 [-1.474, 0.966], mean_best_reward: --\n",
      " 14722/100000: episode: 482, duration: 0.015s, episode steps: 14, steps per second: 906, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.084 [-1.203, 1.830], mean_best_reward: --\n",
      " 14809/100000: episode: 483, duration: 0.085s, episode steps: 87, steps per second: 1026, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.004 [-1.044, 0.912], mean_best_reward: --\n",
      " 14841/100000: episode: 484, duration: 0.025s, episode steps: 32, steps per second: 1270, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.029 [-1.311, 1.905], mean_best_reward: --\n",
      " 14866/100000: episode: 485, duration: 0.021s, episode steps: 25, steps per second: 1168, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.051 [-1.483, 0.785], mean_best_reward: --\n",
      " 14885/100000: episode: 486, duration: 0.017s, episode steps: 19, steps per second: 1130, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.091 [-1.190, 0.594], mean_best_reward: --\n",
      " 14901/100000: episode: 487, duration: 0.014s, episode steps: 16, steps per second: 1146, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.097 [-0.595, 1.185], mean_best_reward: --\n",
      " 14978/100000: episode: 488, duration: 0.069s, episode steps: 77, steps per second: 1123, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.199 [-0.804, 1.608], mean_best_reward: --\n",
      " 14994/100000: episode: 489, duration: 0.016s, episode steps: 16, steps per second: 993, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.076 [-1.295, 0.652], mean_best_reward: --\n",
      " 15043/100000: episode: 490, duration: 0.036s, episode steps: 49, steps per second: 1376, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.070 [-1.937, 0.977], mean_best_reward: --\n",
      " 15067/100000: episode: 491, duration: 0.018s, episode steps: 24, steps per second: 1319, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.051 [-1.782, 1.126], mean_best_reward: --\n",
      " 15102/100000: episode: 492, duration: 0.027s, episode steps: 35, steps per second: 1302, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.067 [-0.605, 1.132], mean_best_reward: --\n",
      " 15129/100000: episode: 493, duration: 0.024s, episode steps: 27, steps per second: 1146, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.081 [-0.536, 0.897], mean_best_reward: --\n",
      " 15144/100000: episode: 494, duration: 0.016s, episode steps: 15, steps per second: 940, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.095 [-0.564, 1.203], mean_best_reward: --\n",
      " 15167/100000: episode: 495, duration: 0.017s, episode steps: 23, steps per second: 1315, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.052 [-1.703, 1.124], mean_best_reward: --\n",
      " 15198/100000: episode: 496, duration: 0.029s, episode steps: 31, steps per second: 1085, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.024 [-1.305, 1.755], mean_best_reward: --\n",
      " 15210/100000: episode: 497, duration: 0.011s, episode steps: 12, steps per second: 1063, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-1.025, 1.782], mean_best_reward: --\n",
      " 15223/100000: episode: 498, duration: 0.015s, episode steps: 13, steps per second: 859, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.097 [-1.463, 0.942], mean_best_reward: --\n",
      " 15252/100000: episode: 499, duration: 0.036s, episode steps: 29, steps per second: 796, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.020 [-1.168, 0.777], mean_best_reward: --\n",
      " 15337/100000: episode: 500, duration: 0.069s, episode steps: 85, steps per second: 1227, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.075 [-0.775, 0.968], mean_best_reward: --\n",
      " 15358/100000: episode: 501, duration: 0.025s, episode steps: 21, steps per second: 857, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.061 [-1.146, 1.859], mean_best_reward: 98.000000\n",
      " 15372/100000: episode: 502, duration: 0.009s, episode steps: 14, steps per second: 1512, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.117 [-0.759, 1.469], mean_best_reward: --\n",
      " 15387/100000: episode: 503, duration: 0.011s, episode steps: 15, steps per second: 1410, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.094 [-0.965, 1.782], mean_best_reward: --\n",
      " 15405/100000: episode: 504, duration: 0.011s, episode steps: 18, steps per second: 1629, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.057 [-1.600, 2.425], mean_best_reward: --\n",
      " 15418/100000: episode: 505, duration: 0.009s, episode steps: 13, steps per second: 1511, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.125 [-0.950, 1.495], mean_best_reward: --\n",
      " 15462/100000: episode: 506, duration: 0.027s, episode steps: 44, steps per second: 1624, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.102 [-1.849, 0.737], mean_best_reward: --\n",
      " 15494/100000: episode: 507, duration: 0.026s, episode steps: 32, steps per second: 1226, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.104 [-0.609, 1.531], mean_best_reward: --\n",
      " 15559/100000: episode: 508, duration: 0.039s, episode steps: 65, steps per second: 1662, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.049 [-1.097, 1.290], mean_best_reward: --\n",
      " 15675/100000: episode: 509, duration: 0.073s, episode steps: 116, steps per second: 1591, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.237 [-2.192, 1.110], mean_best_reward: --\n",
      " 15699/100000: episode: 510, duration: 0.016s, episode steps: 24, steps per second: 1475, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.029 [-1.008, 1.426], mean_best_reward: --\n",
      " 15747/100000: episode: 511, duration: 0.035s, episode steps: 48, steps per second: 1379, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.176 [-0.995, 1.473], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15818/100000: episode: 512, duration: 0.060s, episode steps: 71, steps per second: 1175, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.051 [-1.390, 0.929], mean_best_reward: --\n",
      " 15850/100000: episode: 513, duration: 0.024s, episode steps: 32, steps per second: 1346, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.010 [-1.218, 0.993], mean_best_reward: --\n",
      " 15940/100000: episode: 514, duration: 0.049s, episode steps: 90, steps per second: 1834, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.169 [-1.916, 1.018], mean_best_reward: --\n",
      " 15980/100000: episode: 515, duration: 0.028s, episode steps: 40, steps per second: 1435, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.062 [-0.658, 1.452], mean_best_reward: --\n",
      " 16020/100000: episode: 516, duration: 0.034s, episode steps: 40, steps per second: 1194, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.779, 0.934], mean_best_reward: --\n",
      " 16048/100000: episode: 517, duration: 0.020s, episode steps: 28, steps per second: 1385, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.041 [-0.855, 1.694], mean_best_reward: --\n",
      " 16083/100000: episode: 518, duration: 0.024s, episode steps: 35, steps per second: 1489, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.046 [-1.648, 1.143], mean_best_reward: --\n",
      " 16105/100000: episode: 519, duration: 0.020s, episode steps: 22, steps per second: 1074, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.029 [-1.210, 1.713], mean_best_reward: --\n",
      " 16122/100000: episode: 520, duration: 0.017s, episode steps: 17, steps per second: 982, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.117 [-1.849, 0.945], mean_best_reward: --\n",
      " 16146/100000: episode: 521, duration: 0.016s, episode steps: 24, steps per second: 1484, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.074 [-1.776, 0.984], mean_best_reward: --\n",
      " 16221/100000: episode: 522, duration: 0.050s, episode steps: 75, steps per second: 1511, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.137 [-1.732, 0.794], mean_best_reward: --\n",
      " 16240/100000: episode: 523, duration: 0.015s, episode steps: 19, steps per second: 1305, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.064 [-0.952, 1.694], mean_best_reward: --\n",
      " 16257/100000: episode: 524, duration: 0.015s, episode steps: 17, steps per second: 1115, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.096 [-0.569, 1.269], mean_best_reward: --\n",
      " 16281/100000: episode: 525, duration: 0.025s, episode steps: 24, steps per second: 978, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.077 [-1.574, 0.954], mean_best_reward: --\n",
      " 16321/100000: episode: 526, duration: 0.029s, episode steps: 40, steps per second: 1403, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.789, 1.159], mean_best_reward: --\n",
      " 16335/100000: episode: 527, duration: 0.012s, episode steps: 14, steps per second: 1128, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.094 [-2.040, 1.330], mean_best_reward: --\n",
      " 16347/100000: episode: 528, duration: 0.012s, episode steps: 12, steps per second: 1016, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.092 [-0.830, 1.386], mean_best_reward: --\n",
      " 16397/100000: episode: 529, duration: 0.039s, episode steps: 50, steps per second: 1286, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.022 [-0.945, 1.239], mean_best_reward: --\n",
      " 16412/100000: episode: 530, duration: 0.010s, episode steps: 15, steps per second: 1551, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.084 [-2.209, 1.368], mean_best_reward: --\n",
      " 16448/100000: episode: 531, duration: 0.022s, episode steps: 36, steps per second: 1600, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.006 [-0.759, 1.078], mean_best_reward: --\n",
      " 16479/100000: episode: 532, duration: 0.025s, episode steps: 31, steps per second: 1253, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.112 [-1.324, 0.389], mean_best_reward: --\n",
      " 16530/100000: episode: 533, duration: 0.048s, episode steps: 51, steps per second: 1069, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.068 [-0.460, 1.094], mean_best_reward: --\n",
      " 16572/100000: episode: 534, duration: 0.038s, episode steps: 42, steps per second: 1093, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.003 [-1.122, 0.916], mean_best_reward: --\n",
      " 16588/100000: episode: 535, duration: 0.016s, episode steps: 16, steps per second: 994, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.097 [-1.238, 0.811], mean_best_reward: --\n",
      " 16615/100000: episode: 536, duration: 0.023s, episode steps: 27, steps per second: 1177, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.067 [-1.779, 1.125], mean_best_reward: --\n",
      " 16648/100000: episode: 537, duration: 0.030s, episode steps: 33, steps per second: 1096, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.084 [-1.366, 0.599], mean_best_reward: --\n",
      " 16694/100000: episode: 538, duration: 0.028s, episode steps: 46, steps per second: 1623, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.117 [-1.762, 0.665], mean_best_reward: --\n",
      " 16739/100000: episode: 539, duration: 0.025s, episode steps: 45, steps per second: 1788, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.185 [-0.628, 1.360], mean_best_reward: --\n",
      " 16770/100000: episode: 540, duration: 0.021s, episode steps: 31, steps per second: 1449, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.032 [-1.022, 1.410], mean_best_reward: --\n",
      " 16798/100000: episode: 541, duration: 0.019s, episode steps: 28, steps per second: 1492, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.040 [-1.427, 1.003], mean_best_reward: --\n",
      " 16845/100000: episode: 542, duration: 0.040s, episode steps: 47, steps per second: 1178, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.033 [-0.978, 1.276], mean_best_reward: --\n",
      " 16877/100000: episode: 543, duration: 0.027s, episode steps: 32, steps per second: 1182, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.015 [-0.811, 1.172], mean_best_reward: --\n",
      " 16898/100000: episode: 544, duration: 0.020s, episode steps: 21, steps per second: 1066, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.085 [-1.176, 0.621], mean_best_reward: --\n",
      " 16953/100000: episode: 545, duration: 0.044s, episode steps: 55, steps per second: 1242, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.095 [-1.690, 1.183], mean_best_reward: --\n",
      " 16999/100000: episode: 546, duration: 0.025s, episode steps: 46, steps per second: 1855, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.069 [-1.279, 1.324], mean_best_reward: --\n",
      " 17050/100000: episode: 547, duration: 0.027s, episode steps: 51, steps per second: 1866, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.143 [-0.730, 1.155], mean_best_reward: --\n",
      " 17062/100000: episode: 548, duration: 0.009s, episode steps: 12, steps per second: 1396, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.119 [-1.167, 1.838], mean_best_reward: --\n",
      " 17129/100000: episode: 549, duration: 0.052s, episode steps: 67, steps per second: 1277, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.139 [-1.328, 0.890], mean_best_reward: --\n",
      " 17175/100000: episode: 550, duration: 0.034s, episode steps: 46, steps per second: 1334, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.706, 1.429], mean_best_reward: --\n",
      " 17209/100000: episode: 551, duration: 0.026s, episode steps: 34, steps per second: 1297, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-0.768, 1.312], mean_best_reward: 112.000000\n",
      " 17221/100000: episode: 552, duration: 0.013s, episode steps: 12, steps per second: 959, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.108 [-0.823, 1.410], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17252/100000: episode: 553, duration: 0.025s, episode steps: 31, steps per second: 1258, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.020 [-1.437, 1.119], mean_best_reward: --\n",
      " 17280/100000: episode: 554, duration: 0.023s, episode steps: 28, steps per second: 1214, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.019 [-1.583, 1.204], mean_best_reward: --\n",
      " 17310/100000: episode: 555, duration: 0.018s, episode steps: 30, steps per second: 1654, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-1.369, 0.433], mean_best_reward: --\n",
      " 17343/100000: episode: 556, duration: 0.021s, episode steps: 33, steps per second: 1584, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.087 [-0.668, 1.806], mean_best_reward: --\n",
      " 17379/100000: episode: 557, duration: 0.032s, episode steps: 36, steps per second: 1121, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.032 [-1.355, 2.064], mean_best_reward: --\n",
      " 17397/100000: episode: 558, duration: 0.014s, episode steps: 18, steps per second: 1253, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.060 [-0.820, 1.335], mean_best_reward: --\n",
      " 17413/100000: episode: 559, duration: 0.013s, episode steps: 16, steps per second: 1198, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.084 [-1.359, 2.062], mean_best_reward: --\n",
      " 17444/100000: episode: 560, duration: 0.022s, episode steps: 31, steps per second: 1386, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.088 [-0.768, 1.637], mean_best_reward: --\n",
      " 17510/100000: episode: 561, duration: 0.050s, episode steps: 66, steps per second: 1325, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.083 [-1.318, 1.366], mean_best_reward: --\n",
      " 17527/100000: episode: 562, duration: 0.020s, episode steps: 17, steps per second: 850, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.072 [-0.960, 1.616], mean_best_reward: --\n",
      " 17543/100000: episode: 563, duration: 0.011s, episode steps: 16, steps per second: 1485, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.094 [-0.931, 1.502], mean_best_reward: --\n",
      " 17562/100000: episode: 564, duration: 0.015s, episode steps: 19, steps per second: 1307, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.056 [-0.832, 1.294], mean_best_reward: --\n",
      " 17584/100000: episode: 565, duration: 0.019s, episode steps: 22, steps per second: 1137, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.080 [-0.551, 1.049], mean_best_reward: --\n",
      " 17618/100000: episode: 566, duration: 0.027s, episode steps: 34, steps per second: 1281, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.080 [-1.375, 0.405], mean_best_reward: --\n",
      " 17655/100000: episode: 567, duration: 0.031s, episode steps: 37, steps per second: 1202, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.077 [-0.730, 1.378], mean_best_reward: --\n",
      " 17667/100000: episode: 568, duration: 0.011s, episode steps: 12, steps per second: 1120, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.129 [-1.326, 2.213], mean_best_reward: --\n",
      " 17701/100000: episode: 569, duration: 0.031s, episode steps: 34, steps per second: 1100, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.754, 1.097], mean_best_reward: --\n",
      " 17721/100000: episode: 570, duration: 0.013s, episode steps: 20, steps per second: 1484, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.076 [-1.023, 1.746], mean_best_reward: --\n",
      " 17737/100000: episode: 571, duration: 0.014s, episode steps: 16, steps per second: 1177, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.082 [-1.209, 0.593], mean_best_reward: --\n",
      " 17815/100000: episode: 572, duration: 0.064s, episode steps: 78, steps per second: 1226, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.037 [-1.171, 1.275], mean_best_reward: --\n",
      " 17849/100000: episode: 573, duration: 0.028s, episode steps: 34, steps per second: 1194, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.573, 0.910], mean_best_reward: --\n",
      " 17869/100000: episode: 574, duration: 0.016s, episode steps: 20, steps per second: 1248, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.055 [-1.472, 1.026], mean_best_reward: --\n",
      " 17897/100000: episode: 575, duration: 0.020s, episode steps: 28, steps per second: 1392, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.054 [-0.952, 1.780], mean_best_reward: --\n",
      " 17924/100000: episode: 576, duration: 0.021s, episode steps: 27, steps per second: 1269, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.080 [-0.951, 1.644], mean_best_reward: --\n",
      " 17938/100000: episode: 577, duration: 0.013s, episode steps: 14, steps per second: 1070, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.117 [-1.131, 0.560], mean_best_reward: --\n",
      " 17952/100000: episode: 578, duration: 0.013s, episode steps: 14, steps per second: 1081, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.064 [-1.227, 1.909], mean_best_reward: --\n",
      " 17992/100000: episode: 579, duration: 0.039s, episode steps: 40, steps per second: 1023, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-0.384, 1.226], mean_best_reward: --\n",
      " 18004/100000: episode: 580, duration: 0.012s, episode steps: 12, steps per second: 1001, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.104 [-1.772, 1.181], mean_best_reward: --\n",
      " 18038/100000: episode: 581, duration: 0.025s, episode steps: 34, steps per second: 1384, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.117 [-0.750, 0.402], mean_best_reward: --\n",
      " 18075/100000: episode: 582, duration: 0.028s, episode steps: 37, steps per second: 1328, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.060 [-0.849, 1.455], mean_best_reward: --\n",
      " 18100/100000: episode: 583, duration: 0.015s, episode steps: 25, steps per second: 1651, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.023 [-1.024, 1.594], mean_best_reward: --\n",
      " 18143/100000: episode: 584, duration: 0.023s, episode steps: 43, steps per second: 1833, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.040 [-1.508, 1.619], mean_best_reward: --\n",
      " 18165/100000: episode: 585, duration: 0.014s, episode steps: 22, steps per second: 1518, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.093 [-0.915, 0.633], mean_best_reward: --\n",
      " 18190/100000: episode: 586, duration: 0.023s, episode steps: 25, steps per second: 1103, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.095 [-0.912, 0.615], mean_best_reward: --\n",
      " 18228/100000: episode: 587, duration: 0.030s, episode steps: 38, steps per second: 1287, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.029 [-0.819, 1.427], mean_best_reward: --\n",
      " 18245/100000: episode: 588, duration: 0.015s, episode steps: 17, steps per second: 1120, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.095 [-0.633, 1.354], mean_best_reward: --\n",
      " 18305/100000: episode: 589, duration: 0.050s, episode steps: 60, steps per second: 1202, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.125 [-1.025, 0.543], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18361/100000: episode: 590, duration: 0.056s, episode steps: 56, steps per second: 998, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.203 [-1.075, 0.625], mean_best_reward: --\n",
      " 18395/100000: episode: 591, duration: 0.021s, episode steps: 34, steps per second: 1649, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.424, 1.226], mean_best_reward: --\n",
      " 18426/100000: episode: 592, duration: 0.017s, episode steps: 31, steps per second: 1860, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.054 [-0.787, 1.205], mean_best_reward: --\n",
      " 18442/100000: episode: 593, duration: 0.009s, episode steps: 16, steps per second: 1740, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.088 [-0.755, 1.506], mean_best_reward: --\n",
      " 18503/100000: episode: 594, duration: 0.043s, episode steps: 61, steps per second: 1434, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.035 [-1.190, 1.032], mean_best_reward: --\n",
      " 18521/100000: episode: 595, duration: 0.014s, episode steps: 18, steps per second: 1281, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.061 [-1.017, 1.526], mean_best_reward: --\n",
      " 18574/100000: episode: 596, duration: 0.034s, episode steps: 53, steps per second: 1566, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.024 [-1.701, 1.218], mean_best_reward: --\n",
      " 18597/100000: episode: 597, duration: 0.016s, episode steps: 23, steps per second: 1474, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.091 [-1.485, 0.748], mean_best_reward: --\n",
      " 18665/100000: episode: 598, duration: 0.054s, episode steps: 68, steps per second: 1256, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.063 [-1.547, 1.199], mean_best_reward: --\n",
      " 18703/100000: episode: 599, duration: 0.032s, episode steps: 38, steps per second: 1200, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.004 [-1.013, 1.561], mean_best_reward: --\n",
      " 18740/100000: episode: 600, duration: 0.025s, episode steps: 37, steps per second: 1499, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.137 [-0.451, 0.958], mean_best_reward: --\n",
      " 18756/100000: episode: 601, duration: 0.011s, episode steps: 16, steps per second: 1459, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.126 [-1.648, 0.768], mean_best_reward: 95.500000\n",
      " 18767/100000: episode: 602, duration: 0.009s, episode steps: 11, steps per second: 1208, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.122 [-1.177, 1.950], mean_best_reward: --\n",
      " 18784/100000: episode: 603, duration: 0.012s, episode steps: 17, steps per second: 1422, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.085 [-0.963, 1.622], mean_best_reward: --\n",
      " 18801/100000: episode: 604, duration: 0.013s, episode steps: 17, steps per second: 1342, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.068 [-0.831, 1.229], mean_best_reward: --\n",
      " 18813/100000: episode: 605, duration: 0.009s, episode steps: 12, steps per second: 1396, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.272, 0.817], mean_best_reward: --\n",
      " 18839/100000: episode: 606, duration: 0.016s, episode steps: 26, steps per second: 1654, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.054 [-0.618, 1.329], mean_best_reward: --\n",
      " 18854/100000: episode: 607, duration: 0.010s, episode steps: 15, steps per second: 1451, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.090 [-0.773, 1.441], mean_best_reward: --\n",
      " 18877/100000: episode: 608, duration: 0.016s, episode steps: 23, steps per second: 1419, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.057 [-0.795, 1.130], mean_best_reward: --\n",
      " 18958/100000: episode: 609, duration: 0.057s, episode steps: 81, steps per second: 1433, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.184 [-0.623, 2.298], mean_best_reward: --\n",
      " 18980/100000: episode: 610, duration: 0.017s, episode steps: 22, steps per second: 1269, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.556, 1.156], mean_best_reward: --\n",
      " 19022/100000: episode: 611, duration: 0.028s, episode steps: 42, steps per second: 1507, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.128 [-0.892, 0.543], mean_best_reward: --\n",
      " 19038/100000: episode: 612, duration: 0.013s, episode steps: 16, steps per second: 1231, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.108 [-1.223, 0.629], mean_best_reward: --\n",
      " 19050/100000: episode: 613, duration: 0.008s, episode steps: 12, steps per second: 1478, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.121 [-1.549, 0.929], mean_best_reward: --\n",
      " 19064/100000: episode: 614, duration: 0.010s, episode steps: 14, steps per second: 1432, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.093 [-0.639, 1.278], mean_best_reward: --\n",
      " 19076/100000: episode: 615, duration: 0.010s, episode steps: 12, steps per second: 1189, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.943, 1.590], mean_best_reward: --\n",
      " 19094/100000: episode: 616, duration: 0.017s, episode steps: 18, steps per second: 1039, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.070 [-1.530, 0.824], mean_best_reward: --\n",
      " 19122/100000: episode: 617, duration: 0.024s, episode steps: 28, steps per second: 1144, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.774, 1.555], mean_best_reward: --\n",
      " 19180/100000: episode: 618, duration: 0.056s, episode steps: 58, steps per second: 1045, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.034 [-1.198, 2.107], mean_best_reward: --\n",
      " 19295/100000: episode: 619, duration: 0.111s, episode steps: 115, steps per second: 1034, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.171 [-0.918, 1.502], mean_best_reward: --\n",
      " 19366/100000: episode: 620, duration: 0.055s, episode steps: 71, steps per second: 1283, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.150 [-0.872, 1.222], mean_best_reward: --\n",
      " 19417/100000: episode: 621, duration: 0.030s, episode steps: 51, steps per second: 1690, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.099 [-1.022, 1.437], mean_best_reward: --\n",
      " 19464/100000: episode: 622, duration: 0.049s, episode steps: 47, steps per second: 958, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.035 [-1.346, 0.975], mean_best_reward: --\n",
      " 19505/100000: episode: 623, duration: 0.039s, episode steps: 41, steps per second: 1061, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.129 [-1.177, 0.479], mean_best_reward: --\n",
      " 19522/100000: episode: 624, duration: 0.015s, episode steps: 17, steps per second: 1115, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.100 [-0.762, 1.251], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19537/100000: episode: 625, duration: 0.016s, episode steps: 15, steps per second: 943, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.087 [-0.947, 1.444], mean_best_reward: --\n",
      " 19578/100000: episode: 626, duration: 0.033s, episode steps: 41, steps per second: 1230, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.121 [-0.872, 0.371], mean_best_reward: --\n",
      " 19597/100000: episode: 627, duration: 0.022s, episode steps: 19, steps per second: 865, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.112 [-0.576, 1.151], mean_best_reward: --\n",
      " 19617/100000: episode: 628, duration: 0.021s, episode steps: 20, steps per second: 935, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.028 [-1.375, 1.904], mean_best_reward: --\n",
      " 19781/100000: episode: 629, duration: 0.127s, episode steps: 164, steps per second: 1290, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.069 [-1.336, 1.377], mean_best_reward: --\n",
      " 19794/100000: episode: 630, duration: 0.013s, episode steps: 13, steps per second: 994, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.095 [-1.000, 1.566], mean_best_reward: --\n",
      " 19820/100000: episode: 631, duration: 0.030s, episode steps: 26, steps per second: 876, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.093 [-1.163, 2.331], mean_best_reward: --\n",
      " 19839/100000: episode: 632, duration: 0.015s, episode steps: 19, steps per second: 1268, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.073 [-1.283, 0.775], mean_best_reward: --\n",
      " 19881/100000: episode: 633, duration: 0.038s, episode steps: 42, steps per second: 1109, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.028 [-0.603, 0.831], mean_best_reward: --\n",
      " 19912/100000: episode: 634, duration: 0.034s, episode steps: 31, steps per second: 908, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.088 [-1.179, 0.808], mean_best_reward: --\n",
      " 19929/100000: episode: 635, duration: 0.016s, episode steps: 17, steps per second: 1079, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.077 [-1.145, 1.698], mean_best_reward: --\n",
      " 19981/100000: episode: 636, duration: 0.042s, episode steps: 52, steps per second: 1245, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.673, 1.033], mean_best_reward: --\n",
      " 20002/100000: episode: 637, duration: 0.014s, episode steps: 21, steps per second: 1500, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.069 [-0.810, 1.532], mean_best_reward: --\n",
      " 20017/100000: episode: 638, duration: 0.010s, episode steps: 15, steps per second: 1555, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.106 [-1.528, 0.764], mean_best_reward: --\n",
      " 20036/100000: episode: 639, duration: 0.016s, episode steps: 19, steps per second: 1210, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.086 [-0.777, 1.182], mean_best_reward: --\n",
      " 20095/100000: episode: 640, duration: 0.054s, episode steps: 59, steps per second: 1091, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.095 [-1.449, 0.706], mean_best_reward: --\n",
      " 20112/100000: episode: 641, duration: 0.017s, episode steps: 17, steps per second: 983, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.086 [-1.891, 1.145], mean_best_reward: --\n",
      " 20128/100000: episode: 642, duration: 0.016s, episode steps: 16, steps per second: 1030, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.079 [-0.952, 1.613], mean_best_reward: --\n",
      " 20150/100000: episode: 643, duration: 0.020s, episode steps: 22, steps per second: 1107, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.047 [-1.614, 2.538], mean_best_reward: --\n",
      " 20185/100000: episode: 644, duration: 0.033s, episode steps: 35, steps per second: 1059, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.099 [-0.973, 0.550], mean_best_reward: --\n",
      " 20247/100000: episode: 645, duration: 0.058s, episode steps: 62, steps per second: 1067, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.240 [-0.875, 1.633], mean_best_reward: --\n",
      " 20257/100000: episode: 646, duration: 0.013s, episode steps: 10, steps per second: 778, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.119 [-2.165, 1.370], mean_best_reward: --\n",
      " 20323/100000: episode: 647, duration: 0.045s, episode steps: 66, steps per second: 1480, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.142 [-1.727, 1.509], mean_best_reward: --\n",
      " 20344/100000: episode: 648, duration: 0.015s, episode steps: 21, steps per second: 1365, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.134 [-1.102, 0.553], mean_best_reward: --\n",
      " 20368/100000: episode: 649, duration: 0.017s, episode steps: 24, steps per second: 1420, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.093 [-1.480, 0.610], mean_best_reward: --\n",
      " 20425/100000: episode: 650, duration: 0.037s, episode steps: 57, steps per second: 1554, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.111 [-1.311, 0.821], mean_best_reward: --\n",
      " 20441/100000: episode: 651, duration: 0.011s, episode steps: 16, steps per second: 1460, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.827, 1.233], mean_best_reward: 81.500000\n",
      " 20471/100000: episode: 652, duration: 0.021s, episode steps: 30, steps per second: 1402, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.076 [-0.672, 1.333], mean_best_reward: --\n",
      " 20518/100000: episode: 653, duration: 0.042s, episode steps: 47, steps per second: 1112, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.109 [-1.582, 0.767], mean_best_reward: --\n",
      " 20587/100000: episode: 654, duration: 0.044s, episode steps: 69, steps per second: 1563, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.031 [-1.337, 1.849], mean_best_reward: --\n",
      " 20660/100000: episode: 655, duration: 0.043s, episode steps: 73, steps per second: 1690, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.044 [-0.853, 0.828], mean_best_reward: --\n",
      " 20701/100000: episode: 656, duration: 0.026s, episode steps: 41, steps per second: 1556, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.076 [-0.771, 1.080], mean_best_reward: --\n",
      " 20731/100000: episode: 657, duration: 0.026s, episode steps: 30, steps per second: 1166, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.077 [-1.018, 1.989], mean_best_reward: --\n",
      " 20807/100000: episode: 658, duration: 0.055s, episode steps: 76, steps per second: 1391, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.223 [-0.817, 1.279], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20833/100000: episode: 659, duration: 0.025s, episode steps: 26, steps per second: 1029, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.087 [-0.591, 0.929], mean_best_reward: --\n",
      " 20848/100000: episode: 660, duration: 0.013s, episode steps: 15, steps per second: 1165, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.992, 1.483], mean_best_reward: --\n",
      " 20927/100000: episode: 661, duration: 0.044s, episode steps: 79, steps per second: 1778, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.083 [-0.927, 0.871], mean_best_reward: --\n",
      " 21012/100000: episode: 662, duration: 0.054s, episode steps: 85, steps per second: 1587, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.160 [-1.468, 1.708], mean_best_reward: --\n",
      " 21088/100000: episode: 663, duration: 0.054s, episode steps: 76, steps per second: 1409, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.139 [-1.412, 0.807], mean_best_reward: --\n",
      " 21113/100000: episode: 664, duration: 0.020s, episode steps: 25, steps per second: 1263, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.082 [-0.623, 1.471], mean_best_reward: --\n",
      " 21129/100000: episode: 665, duration: 0.018s, episode steps: 16, steps per second: 890, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.082 [-0.636, 1.313], mean_best_reward: --\n",
      " 21197/100000: episode: 666, duration: 0.043s, episode steps: 68, steps per second: 1589, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: 0.106 [-1.158, 2.182], mean_best_reward: --\n",
      " 21228/100000: episode: 667, duration: 0.021s, episode steps: 31, steps per second: 1457, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.063 [-0.587, 1.080], mean_best_reward: --\n",
      " 21303/100000: episode: 668, duration: 0.063s, episode steps: 75, steps per second: 1191, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.234 [-1.497, 0.617], mean_best_reward: --\n",
      " 21315/100000: episode: 669, duration: 0.012s, episode steps: 12, steps per second: 993, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.117 [-1.168, 2.038], mean_best_reward: --\n",
      " 21349/100000: episode: 670, duration: 0.031s, episode steps: 34, steps per second: 1099, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.032 [-0.600, 1.220], mean_best_reward: --\n",
      " 21375/100000: episode: 671, duration: 0.025s, episode steps: 26, steps per second: 1037, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.748, 1.195], mean_best_reward: --\n",
      " 21385/100000: episode: 672, duration: 0.012s, episode steps: 10, steps per second: 850, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.124 [-2.018, 1.223], mean_best_reward: --\n",
      " 21399/100000: episode: 673, duration: 0.011s, episode steps: 14, steps per second: 1218, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.085 [-0.803, 1.223], mean_best_reward: --\n",
      " 21428/100000: episode: 674, duration: 0.018s, episode steps: 29, steps per second: 1618, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.073 [-0.963, 1.424], mean_best_reward: --\n",
      " 21489/100000: episode: 675, duration: 0.035s, episode steps: 61, steps per second: 1719, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.138 [-1.019, 1.138], mean_best_reward: --\n",
      " 21527/100000: episode: 676, duration: 0.025s, episode steps: 38, steps per second: 1541, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.126 [-0.422, 0.937], mean_best_reward: --\n",
      " 21559/100000: episode: 677, duration: 0.024s, episode steps: 32, steps per second: 1358, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-1.412, 0.939], mean_best_reward: --\n",
      " 21593/100000: episode: 678, duration: 0.030s, episode steps: 34, steps per second: 1147, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.002 [-1.426, 1.129], mean_best_reward: --\n",
      " 21632/100000: episode: 679, duration: 0.028s, episode steps: 39, steps per second: 1384, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.158 [-0.370, 0.762], mean_best_reward: --\n",
      " 21653/100000: episode: 680, duration: 0.013s, episode steps: 21, steps per second: 1656, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.072 [-1.371, 0.801], mean_best_reward: --\n",
      " 21669/100000: episode: 681, duration: 0.011s, episode steps: 16, steps per second: 1459, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.121 [-0.752, 1.550], mean_best_reward: --\n",
      " 21707/100000: episode: 682, duration: 0.031s, episode steps: 38, steps per second: 1220, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.146 [-0.543, 1.378], mean_best_reward: --\n",
      " 21739/100000: episode: 683, duration: 0.020s, episode steps: 32, steps per second: 1567, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.189, 0.466], mean_best_reward: --\n",
      " 21764/100000: episode: 684, duration: 0.016s, episode steps: 25, steps per second: 1526, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.079 [-1.151, 1.960], mean_best_reward: --\n",
      " 21786/100000: episode: 685, duration: 0.015s, episode steps: 22, steps per second: 1474, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.075 [-1.712, 0.965], mean_best_reward: --\n",
      " 21812/100000: episode: 686, duration: 0.021s, episode steps: 26, steps per second: 1228, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.031 [-1.024, 1.624], mean_best_reward: --\n",
      " 21843/100000: episode: 687, duration: 0.027s, episode steps: 31, steps per second: 1144, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.141 [-0.363, 0.851], mean_best_reward: --\n",
      " 21890/100000: episode: 688, duration: 0.050s, episode steps: 47, steps per second: 947, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.062 [-0.729, 1.369], mean_best_reward: --\n",
      " 21952/100000: episode: 689, duration: 0.053s, episode steps: 62, steps per second: 1163, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.070 [-1.591, 1.086], mean_best_reward: --\n",
      " 21972/100000: episode: 690, duration: 0.019s, episode steps: 20, steps per second: 1036, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.551, 0.959], mean_best_reward: --\n",
      " 21997/100000: episode: 691, duration: 0.025s, episode steps: 25, steps per second: 1003, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.107 [-1.255, 0.777], mean_best_reward: --\n",
      " 22045/100000: episode: 692, duration: 0.056s, episode steps: 48, steps per second: 855, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.003 [-0.949, 1.378], mean_best_reward: --\n",
      " 22112/100000: episode: 693, duration: 0.063s, episode steps: 67, steps per second: 1065, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.157 [-1.794, 1.253], mean_best_reward: --\n",
      " 22145/100000: episode: 694, duration: 0.027s, episode steps: 33, steps per second: 1225, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.606 [0.000, 1.000], mean observation: -0.017 [-2.166, 1.502], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22173/100000: episode: 695, duration: 0.024s, episode steps: 28, steps per second: 1170, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.024 [-1.194, 0.829], mean_best_reward: --\n",
      " 22187/100000: episode: 696, duration: 0.012s, episode steps: 14, steps per second: 1211, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.128 [-1.307, 0.749], mean_best_reward: --\n",
      " 22272/100000: episode: 697, duration: 0.046s, episode steps: 85, steps per second: 1862, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.061 [-0.951, 1.166], mean_best_reward: --\n",
      " 22326/100000: episode: 698, duration: 0.038s, episode steps: 54, steps per second: 1425, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.080 [-0.733, 1.103], mean_best_reward: --\n",
      " 22368/100000: episode: 699, duration: 0.039s, episode steps: 42, steps per second: 1072, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.003 [-1.328, 1.813], mean_best_reward: --\n",
      " 22424/100000: episode: 700, duration: 0.047s, episode steps: 56, steps per second: 1195, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.109 [-1.772, 0.921], mean_best_reward: --\n",
      " 22433/100000: episode: 701, duration: 0.011s, episode steps: 9, steps per second: 792, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.113 [-2.099, 1.366], mean_best_reward: 84.000000\n",
      " 22498/100000: episode: 702, duration: 0.050s, episode steps: 65, steps per second: 1311, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.082 [-1.084, 0.790], mean_best_reward: --\n",
      " 22530/100000: episode: 703, duration: 0.022s, episode steps: 32, steps per second: 1437, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.023, 0.369], mean_best_reward: --\n",
      " 22550/100000: episode: 704, duration: 0.016s, episode steps: 20, steps per second: 1274, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.059 [-1.721, 0.994], mean_best_reward: --\n",
      " 22601/100000: episode: 705, duration: 0.036s, episode steps: 51, steps per second: 1432, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.138 [-0.945, 1.160], mean_best_reward: --\n",
      " 22668/100000: episode: 706, duration: 0.053s, episode steps: 67, steps per second: 1270, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.027 [-1.218, 0.973], mean_best_reward: --\n",
      " 22702/100000: episode: 707, duration: 0.033s, episode steps: 34, steps per second: 1026, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.029 [-1.731, 1.509], mean_best_reward: --\n",
      " 22833/100000: episode: 708, duration: 0.097s, episode steps: 131, steps per second: 1353, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.140 [-1.188, 1.650], mean_best_reward: --\n",
      " 22889/100000: episode: 709, duration: 0.042s, episode steps: 56, steps per second: 1318, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.105 [-1.134, 0.783], mean_best_reward: --\n",
      " 22924/100000: episode: 710, duration: 0.027s, episode steps: 35, steps per second: 1294, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.137 [-1.341, 0.733], mean_best_reward: --\n",
      " 22944/100000: episode: 711, duration: 0.014s, episode steps: 20, steps per second: 1479, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.055, 0.613], mean_best_reward: --\n",
      " 22976/100000: episode: 712, duration: 0.020s, episode steps: 32, steps per second: 1628, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.067 [-0.604, 1.194], mean_best_reward: --\n",
      " 23016/100000: episode: 713, duration: 0.032s, episode steps: 40, steps per second: 1253, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.149 [-1.097, 0.612], mean_best_reward: --\n",
      " 23034/100000: episode: 714, duration: 0.018s, episode steps: 18, steps per second: 991, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.095 [-0.571, 1.276], mean_best_reward: --\n",
      " 23052/100000: episode: 715, duration: 0.017s, episode steps: 18, steps per second: 1034, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.101 [-0.600, 1.010], mean_best_reward: --\n",
      " 23075/100000: episode: 716, duration: 0.018s, episode steps: 23, steps per second: 1287, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.052 [-2.055, 1.219], mean_best_reward: --\n",
      " 23095/100000: episode: 717, duration: 0.036s, episode steps: 20, steps per second: 563, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.590, 1.304], mean_best_reward: --\n",
      " 23120/100000: episode: 718, duration: 0.028s, episode steps: 25, steps per second: 897, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.098 [-0.608, 1.099], mean_best_reward: --\n",
      " 23222/100000: episode: 719, duration: 0.129s, episode steps: 102, steps per second: 793, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.111 [-1.045, 0.948], mean_best_reward: --\n",
      " 23289/100000: episode: 720, duration: 0.093s, episode steps: 67, steps per second: 722, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.066 [-0.944, 1.250], mean_best_reward: --\n",
      " 23310/100000: episode: 721, duration: 0.026s, episode steps: 21, steps per second: 793, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.052 [-1.434, 0.818], mean_best_reward: --\n",
      " 23329/100000: episode: 722, duration: 0.014s, episode steps: 19, steps per second: 1394, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.063 [-0.924, 1.373], mean_best_reward: --\n",
      " 23374/100000: episode: 723, duration: 0.027s, episode steps: 45, steps per second: 1694, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.074 [-1.596, 0.794], mean_best_reward: --\n",
      " 23407/100000: episode: 724, duration: 0.022s, episode steps: 33, steps per second: 1530, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.051 [-0.646, 1.347], mean_best_reward: --\n",
      " 23466/100000: episode: 725, duration: 0.043s, episode steps: 59, steps per second: 1359, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.175 [-1.544, 1.544], mean_best_reward: --\n",
      " 23514/100000: episode: 726, duration: 0.039s, episode steps: 48, steps per second: 1217, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.109, 0.640], mean_best_reward: --\n",
      " 23571/100000: episode: 727, duration: 0.042s, episode steps: 57, steps per second: 1355, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.204 [-1.310, 0.589], mean_best_reward: --\n",
      " 23630/100000: episode: 728, duration: 0.043s, episode steps: 59, steps per second: 1359, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: -0.011 [-2.504, 1.404], mean_best_reward: --\n",
      " 23665/100000: episode: 729, duration: 0.033s, episode steps: 35, steps per second: 1061, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.070 [-0.569, 1.137], mean_best_reward: --\n",
      " 23695/100000: episode: 730, duration: 0.029s, episode steps: 30, steps per second: 1037, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.111 [-1.119, 0.731], mean_best_reward: --\n",
      " 23752/100000: episode: 731, duration: 0.050s, episode steps: 57, steps per second: 1134, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.096 [-0.861, 1.071], mean_best_reward: --\n",
      " 23786/100000: episode: 732, duration: 0.029s, episode steps: 34, steps per second: 1179, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-0.578, 1.252], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23815/100000: episode: 733, duration: 0.031s, episode steps: 29, steps per second: 940, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.087 [-0.817, 1.243], mean_best_reward: --\n",
      " 23869/100000: episode: 734, duration: 0.036s, episode steps: 54, steps per second: 1492, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.191 [-1.153, 0.629], mean_best_reward: --\n",
      " 23922/100000: episode: 735, duration: 0.039s, episode steps: 53, steps per second: 1346, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.052 [-1.173, 1.607], mean_best_reward: --\n",
      " 23949/100000: episode: 736, duration: 0.022s, episode steps: 27, steps per second: 1204, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.072 [-0.585, 1.073], mean_best_reward: --\n",
      " 24010/100000: episode: 737, duration: 0.050s, episode steps: 61, steps per second: 1223, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.028 [-0.760, 1.246], mean_best_reward: --\n",
      " 24049/100000: episode: 738, duration: 0.036s, episode steps: 39, steps per second: 1075, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.041 [-0.647, 1.531], mean_best_reward: --\n",
      " 24064/100000: episode: 739, duration: 0.022s, episode steps: 15, steps per second: 680, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.081 [-0.591, 1.098], mean_best_reward: --\n",
      " 24095/100000: episode: 740, duration: 0.032s, episode steps: 31, steps per second: 965, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.113 [-0.939, 0.540], mean_best_reward: --\n",
      " 24125/100000: episode: 741, duration: 0.042s, episode steps: 30, steps per second: 719, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.058 [-0.797, 1.700], mean_best_reward: --\n",
      " 24153/100000: episode: 742, duration: 0.036s, episode steps: 28, steps per second: 775, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.027 [-1.447, 1.016], mean_best_reward: --\n",
      " 24195/100000: episode: 743, duration: 0.061s, episode steps: 42, steps per second: 686, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.059 [-1.136, 1.862], mean_best_reward: --\n",
      " 24265/100000: episode: 744, duration: 0.107s, episode steps: 70, steps per second: 656, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.125 [-0.674, 0.829], mean_best_reward: --\n",
      " 24289/100000: episode: 745, duration: 0.016s, episode steps: 24, steps per second: 1504, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.552, 0.967], mean_best_reward: --\n",
      " 24327/100000: episode: 746, duration: 0.021s, episode steps: 38, steps per second: 1769, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.011 [-1.149, 0.810], mean_best_reward: --\n",
      " 24349/100000: episode: 747, duration: 0.013s, episode steps: 22, steps per second: 1669, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.559, 1.201], mean_best_reward: --\n",
      " 24377/100000: episode: 748, duration: 0.015s, episode steps: 28, steps per second: 1821, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.138 [-0.438, 1.175], mean_best_reward: --\n",
      " 24400/100000: episode: 749, duration: 0.014s, episode steps: 23, steps per second: 1673, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.057 [-0.757, 1.137], mean_best_reward: --\n",
      " 24411/100000: episode: 750, duration: 0.007s, episode steps: 11, steps per second: 1538, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.108 [-1.227, 1.972], mean_best_reward: --\n",
      " 24504/100000: episode: 751, duration: 0.069s, episode steps: 93, steps per second: 1345, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.183 [-1.110, 1.009], mean_best_reward: 115.000000\n",
      " 24538/100000: episode: 752, duration: 0.023s, episode steps: 34, steps per second: 1462, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: -0.095 [-1.844, 0.785], mean_best_reward: --\n",
      " 24583/100000: episode: 753, duration: 0.039s, episode steps: 45, steps per second: 1155, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.151 [-0.903, 0.443], mean_best_reward: --\n",
      " 24629/100000: episode: 754, duration: 0.030s, episode steps: 46, steps per second: 1556, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.034 [-0.622, 1.116], mean_best_reward: --\n",
      " 24644/100000: episode: 755, duration: 0.011s, episode steps: 15, steps per second: 1333, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.104 [-0.542, 1.010], mean_best_reward: --\n",
      " 24672/100000: episode: 756, duration: 0.035s, episode steps: 28, steps per second: 801, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-1.274, 0.942], mean_best_reward: --\n",
      " 24706/100000: episode: 757, duration: 0.033s, episode steps: 34, steps per second: 1021, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.095 [-0.762, 1.236], mean_best_reward: --\n",
      " 24722/100000: episode: 758, duration: 0.020s, episode steps: 16, steps per second: 805, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-1.019, 1.542], mean_best_reward: --\n",
      " 24740/100000: episode: 759, duration: 0.019s, episode steps: 18, steps per second: 923, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.080 [-1.382, 0.750], mean_best_reward: --\n",
      " 24755/100000: episode: 760, duration: 0.017s, episode steps: 15, steps per second: 887, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.065 [-1.190, 1.860], mean_best_reward: --\n",
      " 24765/100000: episode: 761, duration: 0.011s, episode steps: 10, steps per second: 935, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.130 [-1.373, 2.230], mean_best_reward: --\n",
      " 24797/100000: episode: 762, duration: 0.033s, episode steps: 32, steps per second: 956, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-0.984, 0.597], mean_best_reward: --\n",
      " 24817/100000: episode: 763, duration: 0.019s, episode steps: 20, steps per second: 1034, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.045 [-0.827, 1.325], mean_best_reward: --\n",
      " 24846/100000: episode: 764, duration: 0.027s, episode steps: 29, steps per second: 1073, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.116 [-0.638, 1.078], mean_best_reward: --\n",
      " 24859/100000: episode: 765, duration: 0.021s, episode steps: 13, steps per second: 619, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.109 [-1.304, 0.771], mean_best_reward: --\n",
      " 24876/100000: episode: 766, duration: 0.020s, episode steps: 17, steps per second: 858, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.110 [-0.559, 1.338], mean_best_reward: --\n",
      " 24921/100000: episode: 767, duration: 0.051s, episode steps: 45, steps per second: 880, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.103 [-1.278, 0.790], mean_best_reward: --\n",
      " 24933/100000: episode: 768, duration: 0.014s, episode steps: 12, steps per second: 839, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.099 [-1.484, 0.844], mean_best_reward: --\n",
      " 24954/100000: episode: 769, duration: 0.013s, episode steps: 21, steps per second: 1560, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.058 [-1.218, 0.792], mean_best_reward: --\n",
      " 24995/100000: episode: 770, duration: 0.024s, episode steps: 41, steps per second: 1714, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.022 [-1.157, 0.855], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25008/100000: episode: 771, duration: 0.010s, episode steps: 13, steps per second: 1279, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.118 [-1.365, 2.279], mean_best_reward: --\n",
      " 25025/100000: episode: 772, duration: 0.013s, episode steps: 17, steps per second: 1332, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.080 [-0.775, 1.419], mean_best_reward: --\n",
      " 25041/100000: episode: 773, duration: 0.010s, episode steps: 16, steps per second: 1552, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.170, 0.770], mean_best_reward: --\n",
      " 25061/100000: episode: 774, duration: 0.012s, episode steps: 20, steps per second: 1665, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.086 [-0.762, 1.595], mean_best_reward: --\n",
      " 25085/100000: episode: 775, duration: 0.013s, episode steps: 24, steps per second: 1803, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.051 [-0.641, 1.178], mean_best_reward: --\n",
      " 25114/100000: episode: 776, duration: 0.016s, episode steps: 29, steps per second: 1815, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.067 [-1.154, 0.779], mean_best_reward: --\n",
      " 25135/100000: episode: 777, duration: 0.013s, episode steps: 21, steps per second: 1611, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.077 [-0.600, 1.031], mean_best_reward: --\n",
      " 25154/100000: episode: 778, duration: 0.015s, episode steps: 19, steps per second: 1310, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.038 [-1.033, 1.672], mean_best_reward: --\n",
      " 25175/100000: episode: 779, duration: 0.018s, episode steps: 21, steps per second: 1199, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.094 [-0.566, 1.090], mean_best_reward: --\n",
      " 25229/100000: episode: 780, duration: 0.043s, episode steps: 54, steps per second: 1263, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.049 [-1.586, 0.936], mean_best_reward: --\n",
      " 25249/100000: episode: 781, duration: 0.019s, episode steps: 20, steps per second: 1035, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.107 [-0.765, 1.754], mean_best_reward: --\n",
      " 25302/100000: episode: 782, duration: 0.045s, episode steps: 53, steps per second: 1187, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.120 [-0.710, 1.239], mean_best_reward: --\n",
      " 25343/100000: episode: 783, duration: 0.039s, episode steps: 41, steps per second: 1051, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.072 [-1.057, 0.365], mean_best_reward: --\n",
      " 25382/100000: episode: 784, duration: 0.045s, episode steps: 39, steps per second: 865, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.087 [-1.015, 0.580], mean_best_reward: --\n",
      " 25414/100000: episode: 785, duration: 0.033s, episode steps: 32, steps per second: 982, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.027, 0.557], mean_best_reward: --\n",
      " 25439/100000: episode: 786, duration: 0.041s, episode steps: 25, steps per second: 614, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.052 [-0.811, 1.417], mean_best_reward: --\n",
      " 25456/100000: episode: 787, duration: 0.023s, episode steps: 17, steps per second: 738, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.109 [-1.871, 0.963], mean_best_reward: --\n",
      " 25488/100000: episode: 788, duration: 0.044s, episode steps: 32, steps per second: 725, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.057 [-1.489, 0.755], mean_best_reward: --\n",
      " 25507/100000: episode: 789, duration: 0.023s, episode steps: 19, steps per second: 825, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.073 [-1.161, 0.746], mean_best_reward: --\n",
      " 25531/100000: episode: 790, duration: 0.029s, episode steps: 24, steps per second: 837, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.060, 0.567], mean_best_reward: --\n",
      " 25551/100000: episode: 791, duration: 0.013s, episode steps: 20, steps per second: 1572, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.107 [-0.585, 0.983], mean_best_reward: --\n",
      " 25617/100000: episode: 792, duration: 0.039s, episode steps: 66, steps per second: 1700, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.020 [-1.401, 0.763], mean_best_reward: --\n",
      " 25646/100000: episode: 793, duration: 0.019s, episode steps: 29, steps per second: 1560, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.075 [-0.441, 1.027], mean_best_reward: --\n",
      " 25667/100000: episode: 794, duration: 0.015s, episode steps: 21, steps per second: 1395, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.090 [-0.744, 1.382], mean_best_reward: --\n",
      " 25712/100000: episode: 795, duration: 0.027s, episode steps: 45, steps per second: 1641, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.056 [-1.303, 0.436], mean_best_reward: --\n",
      " 25770/100000: episode: 796, duration: 0.032s, episode steps: 58, steps per second: 1787, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.072 [-1.525, 1.335], mean_best_reward: --\n",
      " 25807/100000: episode: 797, duration: 0.028s, episode steps: 37, steps per second: 1306, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.038 [-1.175, 1.230], mean_best_reward: --\n",
      " 25825/100000: episode: 798, duration: 0.025s, episode steps: 18, steps per second: 711, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.108 [-1.375, 0.590], mean_best_reward: --\n",
      " 25884/100000: episode: 799, duration: 0.065s, episode steps: 59, steps per second: 906, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.088 [-1.025, 0.697], mean_best_reward: --\n",
      " 25896/100000: episode: 800, duration: 0.015s, episode steps: 12, steps per second: 804, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.094 [-1.027, 1.659], mean_best_reward: --\n",
      " 25913/100000: episode: 801, duration: 0.028s, episode steps: 17, steps per second: 615, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.077 [-1.857, 1.172], mean_best_reward: 96.500000\n",
      " 25932/100000: episode: 802, duration: 0.026s, episode steps: 19, steps per second: 740, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.102 [-1.308, 0.752], mean_best_reward: --\n",
      " 25945/100000: episode: 803, duration: 0.017s, episode steps: 13, steps per second: 759, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.135 [-0.574, 1.319], mean_best_reward: --\n",
      " 26031/100000: episode: 804, duration: 0.071s, episode steps: 86, steps per second: 1218, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.059 [-0.968, 0.924], mean_best_reward: --\n",
      " 26096/100000: episode: 805, duration: 0.042s, episode steps: 65, steps per second: 1534, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.000 [-0.995, 1.447], mean_best_reward: --\n",
      " 26117/100000: episode: 806, duration: 0.012s, episode steps: 21, steps per second: 1734, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.055 [-1.750, 1.012], mean_best_reward: --\n",
      " 26146/100000: episode: 807, duration: 0.016s, episode steps: 29, steps per second: 1822, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.081 [-0.652, 1.316], mean_best_reward: --\n",
      " 26178/100000: episode: 808, duration: 0.017s, episode steps: 32, steps per second: 1858, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.125 [-0.941, 0.604], mean_best_reward: --\n",
      " 26215/100000: episode: 809, duration: 0.025s, episode steps: 37, steps per second: 1496, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.084 [-0.562, 1.186], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26255/100000: episode: 810, duration: 0.032s, episode steps: 40, steps per second: 1238, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.093 [-0.754, 1.475], mean_best_reward: --\n",
      " 26293/100000: episode: 811, duration: 0.028s, episode steps: 38, steps per second: 1369, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.020 [-1.444, 1.142], mean_best_reward: --\n",
      " 26318/100000: episode: 812, duration: 0.017s, episode steps: 25, steps per second: 1431, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.056 [-1.537, 2.305], mean_best_reward: --\n",
      " 26351/100000: episode: 813, duration: 0.024s, episode steps: 33, steps per second: 1363, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.068 [-1.397, 0.805], mean_best_reward: --\n",
      " 26375/100000: episode: 814, duration: 0.019s, episode steps: 24, steps per second: 1243, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.022 [-1.701, 1.216], mean_best_reward: --\n",
      " 26386/100000: episode: 815, duration: 0.010s, episode steps: 11, steps per second: 1122, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.109 [-1.578, 2.432], mean_best_reward: --\n",
      " 26405/100000: episode: 816, duration: 0.015s, episode steps: 19, steps per second: 1298, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.118 [-0.976, 0.592], mean_best_reward: --\n",
      " 26449/100000: episode: 817, duration: 0.026s, episode steps: 44, steps per second: 1714, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.091 [-0.579, 1.137], mean_best_reward: --\n",
      " 26491/100000: episode: 818, duration: 0.032s, episode steps: 42, steps per second: 1295, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.090 [-1.306, 0.578], mean_best_reward: --\n",
      " 26604/100000: episode: 819, duration: 0.080s, episode steps: 113, steps per second: 1412, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.191 [-1.428, 0.955], mean_best_reward: --\n",
      " 26639/100000: episode: 820, duration: 0.027s, episode steps: 35, steps per second: 1278, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.101 [-0.582, 1.720], mean_best_reward: --\n",
      " 26658/100000: episode: 821, duration: 0.012s, episode steps: 19, steps per second: 1602, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.118 [-0.564, 1.417], mean_best_reward: --\n",
      " 26692/100000: episode: 822, duration: 0.021s, episode steps: 34, steps per second: 1654, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.073 [-0.407, 0.931], mean_best_reward: --\n",
      " 26770/100000: episode: 823, duration: 0.047s, episode steps: 78, steps per second: 1669, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-1.225, 0.859], mean_best_reward: --\n",
      " 26805/100000: episode: 824, duration: 0.028s, episode steps: 35, steps per second: 1265, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.063 [-1.176, 0.446], mean_best_reward: --\n",
      " 26828/100000: episode: 825, duration: 0.017s, episode steps: 23, steps per second: 1379, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.037 [-1.165, 1.597], mean_best_reward: --\n",
      " 26867/100000: episode: 826, duration: 0.029s, episode steps: 39, steps per second: 1340, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.094 [-1.204, 0.439], mean_best_reward: --\n",
      " 26953/100000: episode: 827, duration: 0.060s, episode steps: 86, steps per second: 1444, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-0.969, 1.157], mean_best_reward: --\n",
      " 26970/100000: episode: 828, duration: 0.013s, episode steps: 17, steps per second: 1263, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.080 [-1.249, 0.806], mean_best_reward: --\n",
      " 27012/100000: episode: 829, duration: 0.030s, episode steps: 42, steps per second: 1383, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.169 [-1.563, 0.611], mean_best_reward: --\n",
      " 27040/100000: episode: 830, duration: 0.020s, episode steps: 28, steps per second: 1409, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.626, 1.226], mean_best_reward: --\n",
      " 27076/100000: episode: 831, duration: 0.023s, episode steps: 36, steps per second: 1550, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.055 [-0.628, 0.958], mean_best_reward: --\n",
      " 27111/100000: episode: 832, duration: 0.022s, episode steps: 35, steps per second: 1594, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.147 [-0.368, 1.130], mean_best_reward: --\n",
      " 27147/100000: episode: 833, duration: 0.030s, episode steps: 36, steps per second: 1194, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.455, 1.329], mean_best_reward: --\n",
      " 27177/100000: episode: 834, duration: 0.031s, episode steps: 30, steps per second: 983, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-1.022, 1.414], mean_best_reward: --\n",
      " 27305/100000: episode: 835, duration: 0.089s, episode steps: 128, steps per second: 1444, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-1.119, 1.257], mean_best_reward: --\n",
      " 27403/100000: episode: 836, duration: 0.068s, episode steps: 98, steps per second: 1443, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.261 [-1.518, 1.191], mean_best_reward: --\n",
      " 27427/100000: episode: 837, duration: 0.019s, episode steps: 24, steps per second: 1251, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.198, 0.813], mean_best_reward: --\n",
      " 27445/100000: episode: 838, duration: 0.012s, episode steps: 18, steps per second: 1518, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.079 [-0.845, 1.603], mean_best_reward: --\n",
      " 27472/100000: episode: 839, duration: 0.018s, episode steps: 27, steps per second: 1534, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.085 [-1.293, 0.777], mean_best_reward: --\n",
      " 27546/100000: episode: 840, duration: 0.048s, episode steps: 74, steps per second: 1556, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.150 [-1.219, 0.592], mean_best_reward: --\n",
      " 27572/100000: episode: 841, duration: 0.020s, episode steps: 26, steps per second: 1276, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.153 [-1.324, 0.793], mean_best_reward: --\n",
      " 27609/100000: episode: 842, duration: 0.031s, episode steps: 37, steps per second: 1180, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.061 [-0.601, 0.972], mean_best_reward: --\n",
      " 27646/100000: episode: 843, duration: 0.027s, episode steps: 37, steps per second: 1360, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.149 [-0.438, 1.159], mean_best_reward: --\n",
      " 27661/100000: episode: 844, duration: 0.011s, episode steps: 15, steps per second: 1383, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.102 [-0.797, 1.227], mean_best_reward: --\n",
      " 27680/100000: episode: 845, duration: 0.015s, episode steps: 19, steps per second: 1247, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.075 [-1.812, 0.973], mean_best_reward: --\n",
      " 27692/100000: episode: 846, duration: 0.012s, episode steps: 12, steps per second: 978, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.119 [-0.797, 1.287], mean_best_reward: --\n",
      " 27748/100000: episode: 847, duration: 0.047s, episode steps: 56, steps per second: 1193, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.178 [-1.127, 0.932], mean_best_reward: --\n",
      " 27776/100000: episode: 848, duration: 0.025s, episode steps: 28, steps per second: 1119, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.073 [-0.773, 1.153], mean_best_reward: --\n",
      " 27809/100000: episode: 849, duration: 0.021s, episode steps: 33, steps per second: 1599, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.103 [-0.555, 0.937], mean_best_reward: --\n",
      " 27842/100000: episode: 850, duration: 0.020s, episode steps: 33, steps per second: 1616, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.105 [-1.253, 0.814], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27885/100000: episode: 851, duration: 0.059s, episode steps: 43, steps per second: 732, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.061 [-0.775, 1.734], mean_best_reward: 97.500000\n",
      " 27909/100000: episode: 852, duration: 0.026s, episode steps: 24, steps per second: 930, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.092 [-1.516, 0.644], mean_best_reward: --\n",
      " 27931/100000: episode: 853, duration: 0.027s, episode steps: 22, steps per second: 807, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.089 [-0.583, 1.198], mean_best_reward: --\n",
      " 27976/100000: episode: 854, duration: 0.034s, episode steps: 45, steps per second: 1309, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.092 [-1.255, 0.818], mean_best_reward: --\n",
      " 28024/100000: episode: 855, duration: 0.045s, episode steps: 48, steps per second: 1074, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.593, 1.274], mean_best_reward: --\n",
      " 28034/100000: episode: 856, duration: 0.008s, episode steps: 10, steps per second: 1208, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.103 [-1.010, 1.560], mean_best_reward: --\n",
      " 28144/100000: episode: 857, duration: 0.084s, episode steps: 110, steps per second: 1302, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.174 [-1.694, 0.953], mean_best_reward: --\n",
      " 28169/100000: episode: 858, duration: 0.016s, episode steps: 25, steps per second: 1550, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.055 [-1.382, 2.168], mean_best_reward: --\n",
      " 28210/100000: episode: 859, duration: 0.024s, episode steps: 41, steps per second: 1717, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.128 [-1.116, 0.558], mean_best_reward: --\n",
      " 28235/100000: episode: 860, duration: 0.020s, episode steps: 25, steps per second: 1278, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.084 [-0.579, 0.995], mean_best_reward: --\n",
      " 28278/100000: episode: 861, duration: 0.029s, episode steps: 43, steps per second: 1501, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.098 [-0.642, 0.877], mean_best_reward: --\n",
      " 28369/100000: episode: 862, duration: 0.090s, episode steps: 91, steps per second: 1012, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.012 [-0.962, 1.153], mean_best_reward: --\n",
      " 28439/100000: episode: 863, duration: 0.067s, episode steps: 70, steps per second: 1048, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.109 [-0.994, 1.450], mean_best_reward: --\n",
      " 28452/100000: episode: 864, duration: 0.015s, episode steps: 13, steps per second: 846, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.105 [-1.119, 0.571], mean_best_reward: --\n",
      " 28530/100000: episode: 865, duration: 0.068s, episode steps: 78, steps per second: 1140, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.078 [-1.007, 0.583], mean_best_reward: --\n",
      " 28549/100000: episode: 866, duration: 0.012s, episode steps: 19, steps per second: 1641, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.093 [-0.598, 1.095], mean_best_reward: --\n",
      " 28575/100000: episode: 867, duration: 0.014s, episode steps: 26, steps per second: 1825, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.120 [-0.400, 0.749], mean_best_reward: --\n",
      " 28663/100000: episode: 868, duration: 0.054s, episode steps: 88, steps per second: 1638, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.102 [-1.141, 1.155], mean_best_reward: --\n",
      " 28702/100000: episode: 869, duration: 0.031s, episode steps: 39, steps per second: 1256, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.061 [-0.627, 1.004], mean_best_reward: --\n",
      " 28723/100000: episode: 870, duration: 0.021s, episode steps: 21, steps per second: 995, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.094 [-0.576, 0.937], mean_best_reward: --\n",
      " 28750/100000: episode: 871, duration: 0.019s, episode steps: 27, steps per second: 1443, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.048 [-1.076, 0.610], mean_best_reward: --\n",
      " 28811/100000: episode: 872, duration: 0.038s, episode steps: 61, steps per second: 1625, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.064 [-1.058, 1.131], mean_best_reward: --\n",
      " 28843/100000: episode: 873, duration: 0.022s, episode steps: 32, steps per second: 1477, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.061 [-0.823, 1.722], mean_best_reward: --\n",
      " 28897/100000: episode: 874, duration: 0.039s, episode steps: 54, steps per second: 1383, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.164 [-1.399, 1.082], mean_best_reward: --\n",
      " 28953/100000: episode: 875, duration: 0.044s, episode steps: 56, steps per second: 1278, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.163 [-0.617, 1.292], mean_best_reward: --\n",
      " 28992/100000: episode: 876, duration: 0.033s, episode steps: 39, steps per second: 1200, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.020 [-1.441, 0.839], mean_best_reward: --\n",
      " 29055/100000: episode: 877, duration: 0.043s, episode steps: 63, steps per second: 1459, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.067 [-1.238, 1.137], mean_best_reward: --\n",
      " 29086/100000: episode: 878, duration: 0.019s, episode steps: 31, steps per second: 1626, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.153 [-0.812, 1.094], mean_best_reward: --\n",
      " 29196/100000: episode: 879, duration: 0.085s, episode steps: 110, steps per second: 1288, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.055 [-1.081, 0.947], mean_best_reward: --\n",
      " 29218/100000: episode: 880, duration: 0.014s, episode steps: 22, steps per second: 1533, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.087 [-0.609, 1.283], mean_best_reward: --\n",
      " 29260/100000: episode: 881, duration: 0.028s, episode steps: 42, steps per second: 1511, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.121 [-0.402, 0.893], mean_best_reward: --\n",
      " 29302/100000: episode: 882, duration: 0.036s, episode steps: 42, steps per second: 1162, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.073 [-1.460, 0.575], mean_best_reward: --\n",
      " 29361/100000: episode: 883, duration: 0.053s, episode steps: 59, steps per second: 1109, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.184 [-0.742, 1.193], mean_best_reward: --\n",
      " 29397/100000: episode: 884, duration: 0.031s, episode steps: 36, steps per second: 1171, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.140 [-0.803, 1.164], mean_best_reward: --\n",
      " 29441/100000: episode: 885, duration: 0.026s, episode steps: 44, steps per second: 1660, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.018 [-1.211, 1.106], mean_best_reward: --\n",
      " 29465/100000: episode: 886, duration: 0.018s, episode steps: 24, steps per second: 1325, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.650, 1.332], mean_best_reward: --\n",
      " 29498/100000: episode: 887, duration: 0.023s, episode steps: 33, steps per second: 1421, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.081 [-0.606, 0.927], mean_best_reward: --\n",
      " 29523/100000: episode: 888, duration: 0.022s, episode steps: 25, steps per second: 1138, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.132 [-1.369, 0.755], mean_best_reward: --\n",
      " 29559/100000: episode: 889, duration: 0.023s, episode steps: 36, steps per second: 1593, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.088 [-1.456, 0.895], mean_best_reward: --\n",
      " 29574/100000: episode: 890, duration: 0.013s, episode steps: 15, steps per second: 1199, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.096 [-0.794, 1.250], mean_best_reward: --\n",
      " 29591/100000: episode: 891, duration: 0.015s, episode steps: 17, steps per second: 1168, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.059 [-0.807, 1.286], mean_best_reward: --\n",
      " 29609/100000: episode: 892, duration: 0.018s, episode steps: 18, steps per second: 1007, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.079 [-1.902, 1.144], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29649/100000: episode: 893, duration: 0.037s, episode steps: 40, steps per second: 1081, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-1.252, 0.588], mean_best_reward: --\n",
      " 29684/100000: episode: 894, duration: 0.022s, episode steps: 35, steps per second: 1603, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.111 [-1.165, 0.390], mean_best_reward: --\n",
      " 29768/100000: episode: 895, duration: 0.052s, episode steps: 84, steps per second: 1606, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.195 [-1.653, 0.844], mean_best_reward: --\n",
      " 29790/100000: episode: 896, duration: 0.015s, episode steps: 22, steps per second: 1510, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.397, 1.061], mean_best_reward: --\n",
      " 29815/100000: episode: 897, duration: 0.018s, episode steps: 25, steps per second: 1425, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.055 [-1.349, 0.614], mean_best_reward: --\n",
      " 29835/100000: episode: 898, duration: 0.020s, episode steps: 20, steps per second: 1008, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.112 [-1.197, 0.756], mean_best_reward: --\n",
      " 29864/100000: episode: 899, duration: 0.019s, episode steps: 29, steps per second: 1540, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.117 [-1.335, 0.372], mean_best_reward: --\n",
      " 29890/100000: episode: 900, duration: 0.018s, episode steps: 26, steps per second: 1465, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-0.897, 0.424], mean_best_reward: --\n",
      " 29917/100000: episode: 901, duration: 0.017s, episode steps: 27, steps per second: 1559, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.066 [-0.818, 1.727], mean_best_reward: 101.500000\n",
      " 29957/100000: episode: 902, duration: 0.036s, episode steps: 40, steps per second: 1120, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.105 [-0.854, 1.634], mean_best_reward: --\n",
      " 29974/100000: episode: 903, duration: 0.011s, episode steps: 17, steps per second: 1540, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.099 [-1.337, 0.579], mean_best_reward: --\n",
      " 29988/100000: episode: 904, duration: 0.009s, episode steps: 14, steps per second: 1565, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.122 [-0.764, 1.351], mean_best_reward: --\n",
      " 30022/100000: episode: 905, duration: 0.019s, episode steps: 34, steps per second: 1790, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.085 [-1.148, 0.737], mean_best_reward: --\n",
      " 30060/100000: episode: 906, duration: 0.024s, episode steps: 38, steps per second: 1605, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.162, 0.708], mean_best_reward: --\n",
      " 30071/100000: episode: 907, duration: 0.009s, episode steps: 11, steps per second: 1185, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.115 [-1.603, 1.004], mean_best_reward: --\n",
      " 30086/100000: episode: 908, duration: 0.013s, episode steps: 15, steps per second: 1168, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.108 [-0.753, 1.541], mean_best_reward: --\n",
      " 30111/100000: episode: 909, duration: 0.037s, episode steps: 25, steps per second: 678, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.054 [-1.362, 0.943], mean_best_reward: --\n",
      " 30146/100000: episode: 910, duration: 0.023s, episode steps: 35, steps per second: 1541, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.047 [-0.591, 1.116], mean_best_reward: --\n",
      " 30176/100000: episode: 911, duration: 0.019s, episode steps: 30, steps per second: 1583, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.042 [-1.461, 1.035], mean_best_reward: --\n",
      "done, took 25.836 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 58.000, steps: 58\n",
      "Episode 5: reward: 62.000, steps: 62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b54052490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Option 2: deep network\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(nb_actions))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "cem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the best weights.\n",
    "cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
